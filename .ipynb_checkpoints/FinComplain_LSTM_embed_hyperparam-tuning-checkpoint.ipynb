{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-Parameter Tuning (Embedding model)\n",
    "This document tunes hyperparameters for use in the \"Category embedding model\" that performs text classification of consumer complaints (using NLP-LSTM mode; see script [here](https://github.com/faraway1nspace/NLP_topic_embeddings/blob/master/FinComplain_LSTM_embedding_model.ipynb)), which is an elaboration of a more generic [LSTM-NLP model](https://github.com/faraway1nspace/NLP_topic_embeddings/blob/master/FinComplain_LSTM_default_model.ipynb); the category embedding is inspired by the famous [instacart model](https://tech.instacart.com/deep-learning-with-emojis-not-math-660ba1ad6cdc). The key hyperparameter is the embedding dimension of the category embeddings.\n",
    "\n",
    "<b>Hyperparameters</b> should be done by minimizing a cross-validation loss. In this script, I use a <b>Thompson-sampler</b> (inspired by a cruide _reinforcement-learning_ / multi-arm bandit algorithm), to stochastically explore the high-dimensional hyperparameter space and find good hyperparameter values. This is an original algorithm and is untested, but it seems to work well.\n",
    "\n",
    "The hyperparameters include:\n",
    "+ the embedding dimension of the category embedding\n",
    "+ the embedding dimension of the word-embedding (like word2vec) \n",
    "+ the size of the corpus used in the word-tokenization (which feeds into the word-embedding). \n",
    "+ the dimensionality of the Long-Short-Term-memory outputs\n",
    "+ the Dropout rate in the LSTM\n",
    "\n",
    "... as well as choosing total number of epochs to run the LSTM.\n",
    "\n",
    "### Category Embeddings\n",
    "The category embedding learns a low-dimensional representation of the >400 different financial complaint Issues in the US Financial Consumer Protection [complaint database](https://www.consumerfinance.gov/data-research/consumer-complaints/). The number of embedding dimensions should be much less than the number of categories, at most 1/2, and probably much less than the sqrt(#[categories]). More dimensions means a potentially better representation but higher variance and redundancy; fewer dimensions means more bias but lower variance: it is a classic trade-off, and should be treated as a hyperparameter. If the embeddings have no affect on predictive performance, then this should be reflected in uncertainty in the hyper-parameter tuning algorithm.\n",
    "\n",
    "During an Insight Data fellowship, I used this technique on a similar proprietary dataset for sentiment analysis, and found it help us make sense of the (growing) number of consumer-product categories.\n",
    "\n",
    "### Function overview\n",
    "+ `run_model` The main function which runs an individual cross-validation run \n",
    "+ `proposal_hyperparam` The main thompson sampler function which proposes new samples from the hyperparameter space.\n",
    "\n",
    "The procedure is straight forward\n",
    "+ get <b>samples</b> from the hyperparameter space\n",
    "+ do <b>3-fold cross-validation</b> to estimate an Expected Loss (aka hold-out loss)\n",
    "+ use the Expected Loss as the 'reward' in a <b>multi-arm bandit learner</b>\n",
    "+ calculate <b>probabilities</b> for each combination of hyperparameters\n",
    "+ use the <b>Thompson-sampler</b>/multi-arm bandit algorithm to draw a new sample from the hyperparameter space\n",
    "+ <b>repeat</b> for about 30 iterations.\n",
    "\n",
    "The multi-arm bandit learner should progressively sample from the hyperparameter space that has a higher-probability of minimizing the Expected Loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions: Data Import & Natural Language Pre-Processing\n",
    "\n",
    "The following functions are some idiosyncratic functions to import and clean the Financial complaint data. For the background of the data source and the models' purpose, please see the [Readme file](https://github.com/faraway1nspace/NLP_topic_embeddings) as well as the [US Consumer Complaint Database](https://www.consumerfinance.gov/data-research/consumer-complaints/) at the US Consumer Financial Protection Bureau website.\n",
    "+ `import_and_clean_data` : reads the complaint data and organizes it for the `keras` LSTM model\n",
    "+ `nlp_preprocess` \" does some basic NLP pre-processing (stemming, removing stop words, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# %matplotlib notebook\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from math import log,exp\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import download as nltk_downloader \n",
    "from keras import backend as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Embedding, LSTM, RepeatVector, concatenate, Dense, Reshape, Flatten\n",
    "from keras.models import Model\n",
    "from scipy.stats import rankdata as rd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "from sklearn.model_selection import KFold # import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# set the working directory\n",
    "#os.chdir(\"...\")\n",
    "\n",
    "# NLP function to replace english contractions\n",
    "def decontracted(phrase):\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "# function to do some basic NLP pre-processing steps: replacing contractions, stemming words, removing stop words\n",
    "def nlp_preprocess(text_column, # column in Panda table with text\n",
    "                   stop_words, # list of English stopwords\n",
    "                   word_clip = 300): # truncate the number of words\n",
    "   # remove contractions\n",
    "   ps = PorterStemmer()  # stemmer    \n",
    "   cTextl = [decontracted(x) for x in text_column.values.tolist()]\n",
    "   # remove double spacing and non-alphanumeric characters\n",
    "   cTextl=[re.sub(' +',' ',re.sub(r'\\W+', ' ', x)) for x in cTextl]\n",
    "   # lower case the words\n",
    "   cTextl = [x.lower() for x in cTextl]\n",
    "   # stop words and stemming\n",
    "   for i in range(0,len(cTextl)):\n",
    "      rawtext = cTextl[i].split(\" \") # splits sentence by spaces\n",
    "      rawtext = rawtext[0:min(word_clip,len(rawtext))] # take only 300 words maximum\n",
    "      # stem and remove stopwords in one line (expensive operation)\n",
    "      newtext = \" \".join(ps.stem(word) for word in rawtext if not word in stop_words)  # loop through words, stem,join\n",
    "      cTextl[i] = newtext\n",
    "   return pd.DataFrame(cTextl)\n",
    "\n",
    "# function: import and pre-process the data (prepare for Keras)\n",
    "def import_and_clean_data(filename, # file name of data to import (either a .csv or a tar.xz file of a .csv)\n",
    "                          col_label = \"Label3\",\n",
    "                          data_dir = \"data/\", # directory\n",
    "                          tmp_dir = \"/tmp/\", # if file is a .tar.xz, where to temporarily extract the data (Windows users need specify differently than /tmp/\n",
    "                          rare_categories_cutoff = 10, # threshold for categories to be included in the training set\n",
    "                          word_clip = 300): # max number of words in text to accept (only first 300 words are retained\n",
    "   # check\n",
    "   if \"tar.xz\" in filename:\n",
    "      print(\"decompressing \" + filename + \" into \"+tmp_dir)\n",
    "      # command for shell\n",
    "      os_system_command = \"tar xf \"+data_dir+filename+\" -C \"+tmp_dir\n",
    "      print(os_system_command)\n",
    "      # run decompression command (for Linux/Mac)\n",
    "      os.system(os_system_command)\n",
    "      newfilename = tmp_dir + filename.split(\".tar.xz\")[0]\n",
    "   else:\n",
    "      print(\"importing csv called \" + filename)\n",
    "      newfilename = data_dir + filename\n",
    "   # read the complaint data \n",
    "   d_raw = pd.read_csv(newfilename, usecols = ['State','Complaint ID','Consumer complaint narrative','Product', 'Sub-product', 'Issue', 'Sub-issue'])\n",
    "   print(\"imported \" + str(d_raw.shape[0]) + \" rows of data\") # notice 191829 rows and 7 columns\n",
    "   # fill NaN with blanks\n",
    "   for col_ in ['Product','Sub-product','Issue']:\n",
    "      d_raw[col_] = d_raw[col_].fillna(\" \") # fill NaN with a character\n",
    "   # factorize the two levels (Product and Product+Issue) to get unique values\n",
    "   d_raw['Label1'] = pd.factorize(d_raw['Product'])[0]\n",
    "   # combine Product + Issues\n",
    "   d_raw['Label3'] = pd.factorize(d_raw['Product'] + d_raw['Sub-product']+d_raw['Issue'])[0] # 570 Categories\n",
    "   # Dictionary: category integers vs. category names\n",
    "   cats = [pd.factorize(d_raw['Product'])[1],  pd.factorize(d_raw['Product'] + d_raw['Sub-product'])[1], pd.factorize(d_raw['Product'] + d_raw['Sub-product']+d_raw['Issue'])[1]]\n",
    "   # truncate the data: only use categories with at least 10 observations\n",
    "   labels_counts = d_raw.groupby([col_label]).size() # counts of Level3 categories \n",
    "   which_labels = np.where(labels_counts>=rare_categories_cutoff)[0] # which categories have at least 'cutoff'\n",
    "   # make new (truncated) dataset\n",
    "   ixSubset = d_raw.Label3.isin(which_labels) # subset integers\n",
    "   # new dataset 'd', as subset of d_raw\n",
    "   d = (d_raw[ixSubset]).copy()\n",
    "   # NLP pre-processing: stopwords removal, stemming, etc.\n",
    "   # get the default English stopwords from nlkt pacakge\n",
    "   from nltk.corpus import stopwords \n",
    "   stop_words = set(stopwords.words('english')) # list of stopwords to remove\n",
    "   # get the stemming object from nltk\n",
    "   ps = PorterStemmer()  # stemmer\n",
    "   # column in data with the Text data (to feed into the LSTM)\n",
    "   col_text = 'Consumer complaint narrative' # name of the column with the text \n",
    "   # NLP: pre-process the text/complaints\n",
    "   print(\"performing NLP pre-processing on column \" + col_text + \" (remove stop words, stemming,...). This may take a while...\")\n",
    "   cText = nlp_preprocess(d[col_text],stop_words, word_clip = word_clip)\n",
    "   print(\"Done NLP pre-processing\")\n",
    "   # process the labels, make into a N-hot-coding matrix \n",
    "   Y = pd.get_dummies(d['Label3'].values) # one-hot coding\n",
    "   # get integers representing each (label3) class (these are the column names)\n",
    "   Ynames_int = Y.columns # notice the confusing mapping of different integers to different integers\n",
    "   # get english issue labels corresponding to each integer value in Ynames_int\n",
    "   Ynames_char = [cats[2][i] for i in Ynames_int] # actual names\n",
    "   # Finally, convert Y into a numpy matrix (not a panda df)\n",
    "   Y = Y.values\n",
    "   n_obs = Y.shape[0]\n",
    "   # make the input data for the category embedding\n",
    "   uLabels3 = d['Label3'].unique() # unique level 3 labels\n",
    "   nLabels3 = len(uLabels3) # number of unique level 3 labels\n",
    "   X_labels  = np.repeat(np.array([i for i in range(0,nLabels3)],dtype=int).reshape(1,nLabels3),n_obs,axis=0) # input for embedding layer\n",
    "   print(\"returning cleaned text data 'cText' for use in tokenization; 'Y' as an one-hot-coding matrix of categories; and 'cats' a dictionary matches columns in Y to english category names\")\n",
    "   if \"tar.xz\" in filename:\n",
    "      print(\"deleting temporary file \" + filename)\n",
    "      os_system_command = \"rm \"+ newfilename\n",
    "      print(os_system_command)\n",
    "      os.system(os_system_command)\n",
    "      print(\"done pre-processing\")\n",
    "   return cText, Y, X_labels,cats\n",
    "\n",
    "# function to calculate sample_weights for keras argument sample_weight\n",
    "def get_class_weights(Y, # N-hot-coding response matrix\n",
    "                      clip_ = 100000): # maximum weight for rarer cases\n",
    "   weights_total_class_counts = (Y).sum(axis=0)\n",
    "   weights_by_class = (min(weights_total_class_counts)/weights_total_class_counts) # weight by the rarest case\n",
    "   Y_int = np.argmax(Y,axis=1)\n",
    "   vWeights_raw = np.array([weights_by_class[i] for i in Y_int], dtype=float)\n",
    "   vWeights = np.clip(vWeights_raw * (Y.shape[0]/sum(vWeights_raw)),0,clip_)\n",
    "   return vWeights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions: Hyperparameter Sampling\n",
    "\n",
    "With 4 hyperparameters and (at least) 3 discrete values each, I explored 81 possible _combinations_ of hyperparameter (for an academic or production-level project, you'd likely want more fine-grained hyperparameter values). The goal is to find the best combination of hyperparameter values, based on a cross-validation loss (Expected Loss). It is very inefficient to do cross-validation on EVERY combination. Instead, we'll do a <b>stochastic search</b> across the space of hyperparameter combinations, arriving at the best combination much earlier than running all 81 combinations.\n",
    "\n",
    "#### Thompson sampling:\n",
    "Stochastic exploration of the hyperparameter space requires some way to calculate _probabilities_ for each combination of hyperparameters. I use a simple principle from reinforcement learning called <b>Thompson sampling</b>: the more _uncertain_ a potential action/reward is, the more _likely_ we should try it, and thus reduce our uncertainty quickly find the most rewarding action. Here _action_ means picking a combination of hyperparameters to estimate the Expected Loss; and _reward_ is finding the lowest Expected Loss.\n",
    "\n",
    "#### Model uncertainty as a Dirichlet-Multinomial process\n",
    "The key innovation I present is my method to estimate the probabilities for each hyperparameter combination: this will involve an ensemble of penalized-regressors (`Ridge` and `DecisionTreeRegressor` from `sklearn`). The regressors will estimate the Expected Loss of each combination using the hyperparameter variables as predictor variables. The _frequency counts_ of each hyperparameter-combination being the best model (lowest predicted loss) is converted into a probability via the <b>Dirichlet-Multinomial</b> process. \n",
    "\n",
    "There are a bunch of functions, the main ones pertaining to the hyperparameters are:\n",
    " + `make_hyperparameters_combos` : takes a dictionary of hyperparameters values and makes a grid of all possible combinations\n",
    " + `optimal_order_of_hyperparameter_runs` : proposes an optimal sequence in which to run the different combination of hyperparameter values. This will find high-contrasting parameter-combinations, so that the space of hyperparameter values is quickly explored deterministically, prior to evoking the multi-arm bandit algorithm\n",
    " + `proposal_hyperparam`: main Thompson sampler; proposes new combinations of hyperparameters to run in cross-validation. It switches between a deterministic algorithm and a stochastic algorithm after a preset number of iterations (`toggle_learner`). There are other hyper-hyper-parameters that govern the learning behaviour of the multi-arm-bandit process (`ridge_alpha`, `max_depth`,`multinomial_prior`) and should be left as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# internal function for cross-validation: creates all combinations of hyperparameters\n",
    "def make_hyperparameters_combos(hyper_parameters):\n",
    "   # hyperparmeters' values\n",
    "   hyp_args = [x[1] for x in hyper_parameters.items()]\n",
    "   # names of hyperparameters\n",
    "   hyp_names = [x[0] for x in hyper_parameters.items()]\n",
    "   # all combos of hyperparameters, as a tensor\n",
    "   hyp_args_grid = np.meshgrid(*hyp_args)\n",
    "   # dimensions of the hyperparameters\n",
    "   hyp_args_dimensions = list(hyp_args_grid[0].shape)\n",
    "   # total number of hyperparameter combos\n",
    "   total_combos = round(exp(sum(map(log,hyp_args_dimensions))))\n",
    "   # reshape into a [n_combos,parameters]\n",
    "   hyp_grid = (np.array(hyp_args_grid).reshape(len(hyp_args_grid),total_combos)).T\n",
    "   # convert grid to data.frame\n",
    "   hyp_pd = pd.DataFrame(hyp_grid, columns = hyp_names)\n",
    "   # also make a companion sequence\n",
    "   hyp_seq = [[j for j in range(0,len(x[1]))] for x in hyper_parameters.items()]\n",
    "   hyp_seq_grid = (np.array(np.meshgrid(*hyp_seq)).reshape(len(hyp_args_grid),total_combos)).T\n",
    "   # make an empty panda data.frame to fill with results\n",
    "   empty_res = pd.DataFrame({\"cv_loss\": [0 for i in range(0,hyp_pd.shape[0])], \"best_epoch\": [0 for i in range(0,hyp_pd.shape[0])]})\n",
    "   return hyp_pd, hyp_seq_grid, empty_res\n",
    "\n",
    "# internal function for cross-validation: optimal order of running different hyperparameter scenarios: calculates a 'scenario distance' by finding scenarios that are maximally contrasting with each other\n",
    "def optimal_order_of_hyperparameter_runs(hyp_seq_grid):\n",
    "   scenario_dist = -2 * np.dot(hyp_seq_grid, hyp_seq_grid.T) + np.sum(hyp_seq_grid**2, axis=1) + np.sum(hyp_seq_grid**2, axis=1)[:, np.newaxis]\n",
    "   scenario_rows = [0] # start with row 1\n",
    "   for i in range(0,hyp_seq_grid.shape[0]):\n",
    "      cur_dist = -2 * np.dot(hyp_seq_grid, hyp_seq_grid[scenario_rows].T) + np.sum(hyp_seq_grid[scenario_rows]**2, axis=1) + np.sum(hyp_seq_grid**2, axis=1)[:, np.newaxis]\n",
    "      elem_rank_by_distance = rd(-(((cur_dist**2)).sum(axis=1))**(0.5)).argsort()\n",
    "      pos_elem = [x for x in elem_rank_by_distance if x not in scenario_rows]\n",
    "      if len(pos_elem)>0:\n",
    "         scenario_rows.append(pos_elem[0])\n",
    "   return scenario_rows\n",
    "\n",
    "# internal function for cross-validation: make validation sets and test sets\n",
    "def make_cv_weights(n_obs, fHoldoutProportion = 0.5, kfold = 3, seed = 1000):\n",
    "   # trainging set and test set\n",
    "   ix_train, ix_test = train_test_split([i for i in range(0,n_obs)], test_size = fHoldoutProportion, random_state = seed)\n",
    "   # divide the training data into cross-validation sets\n",
    "   cv_splitter = KFold(n_splits = kfold)\n",
    "   cv_sets = []\n",
    "   for ix_insample, ix_validation in cv_splitter.split(ix_train):\n",
    "      cv_sets.append([ix_insample, ix_validation])\n",
    "   return ix_train, ix_test, cv_sets\n",
    "\n",
    "# internal function for cross-validation: propose hyperparameters, by two methods:\n",
    "# ... i) sequential (just loops through combinations)\n",
    "# ... ii) thompson sampling / multi-arm bandit reinforcement learner (ridge regression & decision trees to try to predict what might be best hyperparameters)\n",
    "def proposal_hyperparam(run_number, # iteration number for the algorithm\n",
    "                        cv_results, # current results of the cv-loss (panda frame)\n",
    "                        hyp_pd, # output from \"make_hyperparameters_combos\" function\n",
    "                        hyp_optimal, # output from \"make_hyperparameters_combos\" function\n",
    "                        toggle_learner = 10, # when to switch to mult-arm bandit learning\n",
    "                        ridge_alpha = 7, # ridge regression learner: shrinkage parameter\n",
    "                        max_depth = 2, # tree-learner maximum tree depth\n",
    "                        multinomial_prior = 1): # diffuse prior on the model space for the multi-arm bandit \n",
    "   n_models = len(hyp_optimal)\n",
    "   if (run_number < toggle_learner) | (run_number > (hyp_pd.shape[0]-1)):\n",
    "      # just sequentially loop through hyperparameter combos\n",
    "      print(\"next hyperparameter: estimated by maximum parameter contrast\")\n",
    "      return_hypIx = hyp_optimal[run_number]\n",
    "      return_hyperparameters = hyp_pd.iloc[return_hypIx]\n",
    "   else:\n",
    "      print(\"next hyperparameter: Thompson sampling of hyperparameters based on a multi-arm bandit learner\") \n",
    "        # find which hyperparam/combos are already completed (used for estimation)\n",
    "      which_done = np.where(cv_results['best_epoch'].values >0)\n",
    "      # find which hyperparam/combos are not yet run (used for predicting their loss)\n",
    "      which_notdone = np.where(cv_results['best_epoch'].values == 0)\n",
    "      # train a ridge regression \n",
    "      learner1 = Ridge(alpha=ridge_alpha, copy_X = True,normalize=True)\n",
    "      # train a decision tree\n",
    "      learner2 = DecisionTreeRegressor(max_depth = max_depth)\n",
    "      # multi-arm bandit: use leave-one-out estimation to get frequency counts that each combo is the best\n",
    "      counts_best_loss = np.zeros(len(which_notdone[0])) + multinomial_prior # notice shrinkage parameter multinomial_prior=1\n",
    "      # loop through each completed combo and drop it (leave-one-out subsampling)\n",
    "      for j in range(0,len(which_done[0])):\n",
    "         # drop the j'th combo (leave-one-out)\n",
    "         loo = which_done[0][np.arange(len(which_done[0]))!=j]\n",
    "         # fit learners\n",
    "         lr1 = learner1.fit(X = hyp_pd.values[loo], y = cv_results['cv_loss'].values[loo]) \n",
    "         lr2 = learner2.fit(X = hyp_pd.values[loo], y = cv_results['cv_loss'].values[loo]) \n",
    "         # predict which will be the lowest loss         \n",
    "         pred_loss1 = lr1.predict(hyp_pd.values[which_notdone]) # expected loss from the learner\n",
    "         pred_loss2 = lr2.predict(hyp_pd.values[which_notdone]) # expected loss from the learner         \n",
    "         # increment number of time's each hypparam/combo is predicted to be the best\n",
    "         counts_best_loss += 0.5*(pred_loss1 == min(pred_loss1))\n",
    "         counts_best_loss += 0.5*(pred_loss2 == min(pred_loss2))\n",
    "      # convert selection frequency of each combo into a probability (Dirichlet)\n",
    "      thompson_sampling_prob = np.random.dirichlet(counts_best_loss)\n",
    "      # Thompson sampler: random sample from the probabilities (Multinomial)\n",
    "      return_hypIx = np.random.choice(which_notdone[0],p=thompson_sampling_prob)\n",
    "      # get the index of the hyperparameters for the best expectd loss\n",
    "      # hyperparameters for the best expectd loss\n",
    "      return_hyperparameters = hyp_pd.iloc[return_hypIx]\n",
    "   return return_hypIx, return_hyperparameters \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions: K-fold Cross-Validation\n",
    "The following functions support the 3-fold cross validation. The are fairly self-explanatory. The main functions are:\n",
    "+ `make_cv_weights` : splits the data into two indices for training & testing. Among the training indices (`ix_train`), it further divides the data into mutually exclusive k-fold cross-validation sets (`cv_sets`). You tune the hyperparameters with the subsets in `cv_sets`; you train a final (tuned) model on the `ix_train`, and you validate the final model on the hold-out set `ix_test`.\n",
    "+ `run_model` : runs an LSTM model using the `keras` API; \n",
    "+ `run_cv_model`: calls `run_model` for one CV-iteration; ONLY returns the validation loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# internal function for cross-validation: make validation sets and test sets\n",
    "def make_cv_weights(n_obs, fHoldoutProportion = 0.5, kfold = 3, seed = 1000):\n",
    "   # trainging set and test set\n",
    "   ix_train, ix_test = train_test_split([i for i in range(0,n_obs)], test_size = fHoldoutProportion, random_state = seed)\n",
    "   # divide the training data into cross-validation sets\n",
    "   cv_splitter = KFold(n_splits = kfold)\n",
    "   cv_sets = []\n",
    "   for ix_insample, ix_validation in cv_splitter.split(ix_train):\n",
    "      cv_sets.append([ix_insample, ix_validation])\n",
    "   return ix_train, ix_test, cv_sets\n",
    "\n",
    "# function: LSTM model, using Keras functional API\n",
    "# use the Keras API to build the model\n",
    "# Main LSTM model. Two inputs: X=word features (tokenized); X_labels=matrix of categories [1,2,3,...,nLabels]\n",
    "# ... the output is Y: one-hot-coding of labels for each row of X,X_labels.\n",
    "# Notice the `RepeatVector` which merges the output from the LSTM with the output from the Category embedding\n",
    "def run_model(X, # tokenized text\n",
    "              X_labels, # categories vector [1,2,3,...]\n",
    "              Y, # labels (one-hot-coding)\n",
    "              W, # sample weights\n",
    "              X_val, # out-of-sample validation: X\n",
    "              X_labels_val, # # out-of-sample validation: X_labels\n",
    "              Y_val, # # out-of-sample validation: Y\n",
    "              max_tokens, # number of words/tokens used to tokenize sentences\n",
    "              n_epoch = 30, # hyperparameter\n",
    "              batch_size = 64, # hyperparameter\n",
    "              dim_embed_lstm= 200, # word embedding dimension\n",
    "              dim_out_lstm =100, # dimension of LSTM output\n",
    "              fDropout_RNN=0.1, # regularization: recurrent_dropout for LSTM\n",
    "              fDropout=0.33, # regularization: input dropout for LSTM\n",
    "              dim_embed_categ=5, # embedding dimension of the categories\n",
    "              dim_hidden_nodes_final =100): # number of hidden nodes in final Dense layer\n",
    "   nLabels = X_labels.shape[1]\n",
    "   # main input: the word features (tokenized) \n",
    "   lstm_input_layer = Input(shape=(X.shape[1],), dtype='int32', name='lstm_input',) # lstm input\n",
    "   # word embedding (like word2vec)\n",
    "   lstm_embed_layer = Embedding(input_dim=max_tokens, output_dim=dim_embed_lstm, input_length=X.shape[1])(lstm_input_layer)\n",
    "   lstm_output = LSTM(dim_out_lstm, dropout = fDropout, recurrent_dropout = fDropout_RNN)(lstm_embed_layer)\n",
    "   # reshape the LSTM output to concatenate with the category embedding\n",
    "   lstm_reshape = RepeatVector(nLabels)(lstm_output)  \n",
    "   label3_input_layer = Input(shape=(X_labels.shape[1],), dtype='int32', name='label3_input') # input the topics\n",
    "   label3_embed_layer = Embedding(input_dim=nLabels, output_dim = dim_embed_categ, input_length=X_labels.shape[1])(label3_input_layer) # topic embedding: should have dim: None,7,embed_dim\n",
    "   # merge the LSTM output with the category embedding\n",
    "   x = concatenate([lstm_reshape,label3_embed_layer],axis=2) # \n",
    "   # final hidden layer\n",
    "   hidden_layer = Dense(dim_hidden_nodes_final, activation='relu')(x)\n",
    "   final_layer = Dense(1, activation='sigmoid')(hidden_layer) # main output for categories\n",
    "   # reshape the output so that it is multinomial in [n_obs, n_categories]\n",
    "   final_layer2 = Flatten()(final_layer) # dimension: [n_obs, n_categories]\n",
    "   main_output = Dense(Y.shape[1], activation='softmax',name = 'main_output')(final_layer2) # main output for categories \n",
    "   model = Model(inputs=[lstm_input_layer, label3_input_layer], outputs=[main_output])\n",
    "   model.compile(loss = \"categorical_crossentropy\", optimizer='adam') \n",
    "   history = model.fit({'lstm_input': X, 'label3_input': X_labels }, {'main_output': Y}, epochs = n_epoch, batch_size=batch_size, verbose = 0, sample_weight = W, validation_data=([X_val, X_labels_val], Y_val))\n",
    "   return model, history\n",
    "\n",
    "# main function: run individual cv-run; ONLY returns the validation loss (deletes model)\n",
    "def run_cv_model(cv_insample_indices,cv_validation_indices, Y, cText, X_labels, vWeights, hyperparam, n_epoch = 7, fDropout_RNN = 0.1, batch_size = 64):\n",
    "   n_obs = Y.shape[0] # number of observations/rows\n",
    "   # get hyperparameters\n",
    "   iMaxTokens = int(hyperparam['max_tokens']) # maximum number of words in corpus for embedding\n",
    "   iDimEmbedLSTM = int(hyperparam['DimEmbedLSTM']) # embedding dimensions for words\n",
    "   iDimOutLSTM = int(hyperparam['DimOutLSTM']) # LSTM output dimension\n",
    "   fDropout = hyperparam['Dropout'] # LSTM input dropout regularization\n",
    "   iDimEmbedCategory = int(hyperparam['DimEmbedCategory'])\n",
    "   iDim_hidden_nodes_final = (np.linspace(iDimOutLSTM,Y.shape[1]).round().astype(int))[1] # number of    \n",
    "   # tokenize the text\n",
    "   tokenizer = Tokenizer(num_words=iMaxTokens, split=' ')\n",
    "   tokenizer.fit_on_texts(\"STARTCODON \" + cText[0])\n",
    "   # text data: insample and validation\n",
    "   X = pad_sequences(tokenizer.texts_to_sequences((\"STARTCODON \" + cText[0]).values)) # tokenize and pad with zeros\n",
    "   X_insample = X[cv_insample_indices] \n",
    "   X_val = X[cv_validation_indices]\n",
    "   # label data (N-hot-coding): insample and validation\n",
    "   Y_insample = Y[cv_insample_indices]\n",
    "   Y_val = Y[cv_validation_indices]\n",
    "   # labels for category embedding\n",
    "   X_labels_insample = X_labels[cv_insample_indices]\n",
    "   X_labels_val = X_labels[cv_validation_indices]\n",
    "   # training weights\n",
    "   W_insample = vWeights[cv_insample_indices]\n",
    "   # fit the model\n",
    "   model, history = run_model(X_insample, X_labels_insample, Y_insample, W_insample, \n",
    "                              X_val,X_labels_val,Y_val, \n",
    "                              iMaxTokens, \n",
    "                              n_epoch, \n",
    "                              batch_size, \n",
    "                              iDimEmbedLSTM, \n",
    "                              iDimOutLSTM, \n",
    "                              fDropout_RNN, \n",
    "                              fDropout, \n",
    "                              iDimEmbedCategory, \n",
    "                              iDim_hidden_nodes_final)\n",
    "   val_loss = history.history['val_loss']\n",
    "   del model\n",
    "   del history\n",
    "   return val_loss\n",
    "\n",
    "# internal function for cross-validation: get the best epoch over k-fold validation\n",
    "def get_best_epoch(cv_losses):\n",
    "      expected_loss_per_epoch = (np.array(cv_losses).T).mean(axis=1)\n",
    "      best_epoch = expected_loss_per_epoch.argmin() + 1\n",
    "      return best_epoch, expected_loss_per_epoch[best_epoch-1]\n",
    "\n",
    "# internal function for cross-validation: get best hyperparameter combinations from CV-loss results\n",
    "def get_best_hyperparameters(cv_results, hyperparameter_grid):\n",
    "   which_done = np.where(cv_results['best_epoch'].values!=0)\n",
    "   which_best = which_done[0][cv_results['cv_loss'].values[which_done[0]].argmin()]\n",
    "   best_epoch = cv_results['best_epoch'][which_best]\n",
    "   return hyperparameter_grid.iloc[which_best], best_epoch \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script: Part 1: import and process data\n",
    "\n",
    "+ NLP the text data (`cText`). \n",
    "+ Convert the categories/labels into one-hot-coding (`Y`)\n",
    "+ Make the category embedding (`X_labels3`).\n",
    "+ Make sample weights (`vWeights`) for the keras `sample_weight` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decompressing complaints-2018-09-30_17_53.csv.tar.xz into /tmp/\n",
      "tar xf data/complaints-2018-09-30_17_53.csv.tar.xz -C /tmp/\n",
      "imported 191828 rows of data\n",
      "performing NLP pre-processing on column Consumer complaint narrative (remove stop words, stemming,...). This may take a while...\n",
      "Done NLP pre-processing\n",
      "returning cleaned text data 'cText' for use in tokenization; 'Y' as an one-hot-coding matrix of categories; and 'cats' a dictionary matches columns in Y to english category names\n",
      "deleting temporary file complaints-2018-09-30_17_53.csv.tar.xz\n",
      "rm /tmp/complaints-2018-09-30_17_53.csv\n",
      "done pre-processing\n",
      "number of observations:191193\n",
      "number of categories:425\n"
     ]
    }
   ],
   "source": [
    "# import and clean the data.\n",
    "# Returns 'cText' (the NLP-preprocessed data) and 'cats' (the dictionary of categories/labels for classification')\n",
    "cText, Y, X_labels3, cats = import_and_clean_data(filename = \"complaints-2018-09-30_17_53.csv.tar.xz\", # filename to import the data,\n",
    "    #filename = \"complaints-2018-09-30_17_53.csv\", # filename to import the data,     \n",
    "                                    col_label = \"Label3\", # label to work with for classification/learning\n",
    "                                    data_dir = \"data/\", # directory\n",
    "                                    tmp_dir = \"/tmp/\", # if file is a .tar.xz, where to temporarily extract the data (Windows users need specify differently than /tmp/\n",
    "                                    rare_categories_cutoff = 10, # threshold for categories to be included in the training set\n",
    "                                    word_clip = 300) # max number of words in text to accept (only first 300 words are retained\n",
    "\n",
    "n_obs = Y.shape[0]\n",
    "nLabels3 = X_labels3.shape[1]\n",
    "print(\"number of observations:\" + str(n_obs))\n",
    "print(\"number of categories:\" + str(nLabels3))\n",
    "      \n",
    "# sample weights to correct for unbalanced categories / labels\n",
    "vWeights = get_class_weights(Y,5) # weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script: Part 2: define the hyperparameters and set-up cross-validation\n",
    "### validation sets\n",
    "+ `kfold` : used for k-fold cross-validation; I use 3-fold validation\n",
    "+ `fHoldoutProportion` : used to define the proportion of data not-used for training, but for validating the final model.\n",
    "\n",
    "### hyperparameter space\n",
    "+ `max_tokens`: the size of the corpus used in the word-tokenization (which feeds into the word-embedding).\n",
    "+ `DimEmbedLSTM`:the embedding dimension of the word-embedding (like word2vec) \n",
    "+ `DimOutLSTM` : the dimensionality of the Long-Short-Term-memory outputs\n",
    "+ `Dropout` regularization parameter: dropout rate in the LSTM\n",
    "+ `DimEmbedCategory`: the embedding dimension of the category-embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = 3 # k-fold cross-validation\n",
    "fHoldoutProportion = 0.8 # amount of data to leave aside for (final) hold-out validation\n",
    "\n",
    "# hyperparmeters' and their values\n",
    "hyper_parameters = {'max_tokens': [1500,2000,3000], # number of wor tokens\n",
    "                    'DimEmbedLSTM':[97,194,291], # word embedding dimension\n",
    "                    'DimOutLSTM':[100,150,300], # LSTM output dimension\n",
    "                    'Dropout':[0.20,0.33,0.5], # dropout proportion for LSTM\n",
    "                    'DimEmbedCategory':[2,4,8,12,16,20]} # category embedding\n",
    "\n",
    "# hyperparameters: make all possible combinations of \n",
    "hyp_grid, ix_hyp_grid, cv_results = make_hyperparameters_combos(hyper_parameters)\n",
    "optimal_hyp_order = optimal_order_of_hyperparameter_runs(ix_hyp_grid)\n",
    "\n",
    "# training set, testing sets, and cross-validation sets: indicies to divide the data\n",
    "ix_train,ix_test,cv_sets = make_cv_weights(n_obs, fHoldoutProportion = fHoldoutProportion, kfold = kfold, seed = 1000)\n",
    "\n",
    "n_epoch = 25 # number of epochs\n",
    "batch_size = 200 # batchsize\n",
    "pause_seconds = 120 # pause between CV-iterations\n",
    "\n",
    "# big loop to run through hyperparameters\n",
    "max_hyperparameter_runs = min(20, len(optimal_hyp_order)) # maximum number of iterations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script: Part 3: Explore the hyperparameter space (cross-validation)\n",
    "This is the main part of the exercise, consisting of two massive loops. The outer loop iterates through different hyperparameter values, and the inner loop iterates through the K-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running k-fold hyperparameter estimation algorithm for 20iterations\n",
      "hyperparameter tuning, iteration: 0\n",
      "next hyperparameter: estimated by maximum parameter contrast\n",
      "[1.5e+03 9.7e+01 1.0e+02 2.0e-01 2.0e+00]\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "lstm_input (InputLayer)         (None, 301)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 301, 97)      145500      lstm_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 100)          79200       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "label3_input (InputLayer)       (None, 425)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 425, 100)     0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 425, 2)       850         label3_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 425, 102)     0           repeat_vector_1[0][0]            \n",
      "                                                                 embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 425, 107)     11021       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 425, 1)       108         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 425)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "main_output (Dense)             (None, 425)          181050      flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 417,729\n",
      "Trainable params: 417,729\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "lstm_input (InputLayer)         (None, 301)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 301, 97)      145500      lstm_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 100)          79200       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "label3_input (InputLayer)       (None, 425)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_2 (RepeatVector)  (None, 425, 100)     0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 425, 2)       850         label3_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 425, 102)     0           repeat_vector_2[0][0]            \n",
      "                                                                 embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 425, 107)     11021       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 425, 1)       108         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 425)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "main_output (Dense)             (None, 425)          181050      flatten_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 417,729\n",
      "Trainable params: 417,729\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "lstm_input (InputLayer)         (None, 301)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 301, 97)      145500      lstm_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 100)          79200       embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "label3_input (InputLayer)       (None, 425)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_3 (RepeatVector)  (None, 425, 100)     0           lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 425, 2)       850         label3_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 425, 102)     0           repeat_vector_3[0][0]            \n",
      "                                                                 embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 425, 107)     11021       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 425, 1)       108         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 425)          0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "main_output (Dense)             (None, 425)          181050      flatten_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 417,729\n",
      "Trainable params: 417,729\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done iteration 0. Sleeping for 120 seconds.\n",
      "hyperparameter tuning, iteration: 1\n",
      "next hyperparameter: estimated by maximum parameter contrast\n",
      "[3.00e+03 2.91e+02 3.00e+02 5.00e-01 2.00e+01]\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "lstm_input (InputLayer)         (None, 301)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 301, 291)     873000      lstm_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 300)          710400      embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "label3_input (InputLayer)       (None, 425)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_4 (RepeatVector)  (None, 425, 300)     0           lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 425, 20)      8500        label3_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 425, 320)     0           repeat_vector_4[0][0]            \n",
      "                                                                 embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 425, 303)     97263       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 425, 1)       304         dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 425)          0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "main_output (Dense)             (None, 425)          181050      flatten_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,870,517\n",
      "Trainable params: 1,870,517\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "lstm_input (InputLayer)         (None, 301)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 301, 291)     873000      lstm_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   (None, 300)          710400      embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "label3_input (InputLayer)       (None, 425)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_5 (RepeatVector)  (None, 425, 300)     0           lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 425, 20)      8500        label3_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 425, 320)     0           repeat_vector_5[0][0]            \n",
      "                                                                 embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 425, 303)     97263       concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 425, 1)       304         dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 425)          0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "main_output (Dense)             (None, 425)          181050      flatten_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,870,517\n",
      "Trainable params: 1,870,517\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "lstm_input (InputLayer)         (None, 301)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, 301, 291)     873000      lstm_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   (None, 300)          710400      embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "label3_input (InputLayer)       (None, 425)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_6 (RepeatVector)  (None, 425, 300)     0           lstm_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)        (None, 425, 20)      8500        label3_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 425, 320)     0           repeat_vector_6[0][0]            \n",
      "                                                                 embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 425, 303)     97263       concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 425, 1)       304         dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 425)          0           dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "main_output (Dense)             (None, 425)          181050      flatten_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,870,517\n",
      "Trainable params: 1,870,517\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rob/.local/lib/python3.6/site-packages/pandas/core/indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done iteration 1. Sleeping for 120 seconds.\n",
      "hyperparameter tuning, iteration: 2\n",
      "next hyperparameter: estimated by maximum parameter contrast\n",
      "[1.50e+03 1.94e+02 1.00e+02 2.00e-01 2.00e+00]\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "lstm_input (InputLayer)         (None, 301)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)        (None, 301, 194)     291000      lstm_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   (None, 100)          118000      embedding_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "label3_input (InputLayer)       (None, 425)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_7 (RepeatVector)  (None, 425, 100)     0           lstm_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)        (None, 425, 2)       850         label3_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 425, 102)     0           repeat_vector_7[0][0]            \n",
      "                                                                 embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 425, 107)     11021       concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 425, 1)       108         dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 425)          0           dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "main_output (Dense)             (None, 425)          181050      flatten_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 602,029\n",
      "Trainable params: 602,029\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "lstm_input (InputLayer)         (None, 301)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_15 (Embedding)        (None, 301, 194)     291000      lstm_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   (None, 100)          118000      embedding_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "label3_input (InputLayer)       (None, 425)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_8 (RepeatVector)  (None, 425, 100)     0           lstm_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_16 (Embedding)        (None, 425, 2)       850         label3_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 425, 102)     0           repeat_vector_8[0][0]            \n",
      "                                                                 embedding_16[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 425, 107)     11021       concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 425, 1)       108         dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 425)          0           dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "main_output (Dense)             (None, 425)          181050      flatten_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 602,029\n",
      "Trainable params: 602,029\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "lstm_input (InputLayer)         (None, 301)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_17 (Embedding)        (None, 301, 194)     291000      lstm_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_9 (LSTM)                   (None, 100)          118000      embedding_17[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "label3_input (InputLayer)       (None, 425)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_9 (RepeatVector)  (None, 425, 100)     0           lstm_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_18 (Embedding)        (None, 425, 2)       850         label3_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 425, 102)     0           repeat_vector_9[0][0]            \n",
      "                                                                 embedding_18[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 425, 107)     11021       concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 425, 1)       108         dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 425)          0           dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "main_output (Dense)             (None, 425)          181050      flatten_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 602,029\n",
      "Trainable params: 602,029\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done iteration 2. Sleeping for 120 seconds.\n",
      "hyperparameter tuning, iteration: 3\n",
      "next hyperparameter: estimated by maximum parameter contrast\n",
      "[3.0e+03 9.7e+01 3.0e+02 5.0e-01 2.0e+01]\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "lstm_input (InputLayer)         (None, 301)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_19 (Embedding)        (None, 301, 97)      291000      lstm_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                  (None, 300)          477600      embedding_19[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "label3_input (InputLayer)       (None, 425)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_10 (RepeatVector) (None, 425, 300)     0           lstm_10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_20 (Embedding)        (None, 425, 20)      8500        label3_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 425, 320)     0           repeat_vector_10[0][0]           \n",
      "                                                                 embedding_20[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 425, 303)     97263       concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 425, 1)       304         dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 425)          0           dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "main_output (Dense)             (None, 425)          181050      flatten_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,055,717\n",
      "Trainable params: 1,055,717\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "lstm_input (InputLayer)         (None, 301)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_21 (Embedding)        (None, 301, 97)      291000      lstm_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_11 (LSTM)                  (None, 300)          477600      embedding_21[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "label3_input (InputLayer)       (None, 425)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_11 (RepeatVector) (None, 425, 300)     0           lstm_11[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_22 (Embedding)        (None, 425, 20)      8500        label3_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 425, 320)     0           repeat_vector_11[0][0]           \n",
      "                                                                 embedding_22[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 425, 303)     97263       concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 425, 1)       304         dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 425)          0           dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "main_output (Dense)             (None, 425)          181050      flatten_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,055,717\n",
      "Trainable params: 1,055,717\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-cadd930c7d56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m    \u001b[0;31m# inner loop, cross-validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m    \u001b[0;32mfor\u001b[0m \u001b[0micv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlcv_set\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_sets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m       \u001b[0mk_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_cv_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlcv_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlcv_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcText\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_labels3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvWeights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_hyper_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfDropout_RNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m    \u001b[0;31m# return best mean(validation_loss) at which epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m    \u001b[0mbest_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_expected_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_best_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_loss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# get best epoch and expected loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-10f336d2b9a3>\u001b[0m in \u001b[0;36mrun_cv_model\u001b[0;34m(cv_insample_indices, cv_validation_indices, Y, cText, X_labels, vWeights, hyperparam, n_epoch, fDropout_RNN, batch_size)\u001b[0m\n\u001b[1;32m     91\u001b[0m                               \u001b[0mfDropout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                               \u001b[0miDimEmbedCategory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                               iDim_hidden_nodes_final)\n\u001b[0m\u001b[1;32m     94\u001b[0m    \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m    \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-10f336d2b9a3>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(X, X_labels, Y, W, X_val, X_labels_val, Y_val, max_tokens, n_epoch, batch_size, dim_embed_lstm, dim_out_lstm, fDropout_RNN, fDropout, dim_embed_categ, dim_hidden_nodes_final)\u001b[0m\n\u001b[1;32m     52\u001b[0m    \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"categorical_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m    \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m    \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'lstm_input'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'label3_input'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_labels\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'main_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_labels_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1395\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1396\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1397\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1398\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1399\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i = 0 # iterator, outer loop\n",
    "print(\"running k-fold hyperparameter estimation algorithm for \" + str(max_hyperparameter_runs) + \"iterations\")\n",
    "while i < max_hyperparameter_runs:\n",
    "   print(\"hyperparameter tuning, iteration: \" + str(i))\n",
    "   # proposal new hyperparameters from the space of combinations\n",
    "   ixHyper, current_hyper_param = proposal_hyperparam(i, cv_results, hyp_pd = hyp_grid, hyp_optimal = optimal_hyp_order, toggle_learner = 10, ridge_alpha = 7)\n",
    "   print(current_hyper_param.values)\n",
    "   # do cross-validation\n",
    "   k_loss = [] # validation loss results\n",
    "   # inner loop, cross-validation\n",
    "   for icv, lcv_set in enumerate(cv_sets):\n",
    "      k_loss.append(run_cv_model(lcv_set[0],lcv_set[1], Y, cText, X_labels3, vWeights, current_hyper_param, n_epoch = n_epoch, fDropout_RNN = 0.1, batch_size = batch_size))\n",
    "   # return best mean(validation_loss) at which epoch \n",
    "   best_epoch, k_expected_loss = get_best_epoch(k_loss) # get best epoch and expected loss\n",
    "   cv_results['best_epoch'].iloc[ixHyper] = best_epoch\n",
    "   cv_results['cv_loss'].iloc[ixHyper] = k_expected_loss\n",
    "   print(\"done iteration \"+ str(i)+ \". Sleeping for \" + str(pause_seconds) +\" seconds.\")\n",
    "   time.sleep(pause_seconds)\n",
    "   # repeat\n",
    "   i += 1\n",
    "\n",
    "print(\"done k-fold hyperparameter estimation: hit \" + str(max_hyperparameter_runs) + \" iterations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script: Part 4: Get best hyperparameters (tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the CV results: get best hyperparameters\n",
    "best_hyperparameters, n_epoch = get_best_hyperparameters(cv_results, hyp_grid)\n",
    "cv_results = pd.concat([pd.DataFrame(hyp_grid),cv_results],axis=1)\n",
    "cv_results = cv_results[cv_results['best_epoch']!=0]\n",
    "cv_results.sort_values(by=['cv_loss'])\n",
    "print(cv_results) # print the results of cross-validation\n",
    "\n",
    "# Final best hyperparameters\n",
    "iMaxTokens = int(best_hyperparameters['max_tokens'])\n",
    "iDimEmbedLSTM = int(best_hyperparameters['DimEmbedLSTM'])\n",
    "iDimOutLSTM = int(best_hyperparameters['DimOutLSTM'])\n",
    "fDropout = best_hyperparameters['Dropout']\n",
    "iDim_hidden_nodes_final = (np.linspace(iDimOutLSTM,Y.shape[1]).round().astype(int))[1] # number of\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script: Part 5: Run the final 'tuned' model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit final model\n",
    "tokenizer = Tokenizer(num_words=iMaxTokens, split=' ')\n",
    "tokenizer.fit_on_texts(\"STARTCODON \" + cText[0])\n",
    "# text data: insample and validation\n",
    "X = pad_sequences(tokenizer.texts_to_sequences((\"STARTCODON \" + cText[0]).values)) # tokenize and pad with zeros\n",
    "X_train = X[ix_train];\n",
    "Y_train = Y[ix_train]\n",
    "W_train = vWeights[ix_train]\n",
    "# final model\n",
    "lstm_input_layer = Input(shape=(X_train.shape[1],), dtype='int32', name='lstm_input',) # lstm input\n",
    "lstm_embed_layer = Embedding(input_dim=iMaxTokens, output_dim=iDimEmbedLSTM, input_length=X_train.shape[1])(lstm_input_layer) # input_dim = vocab size,\n",
    "lstm_output = LSTM(iDimOutLSTM, dropout = fDropout, recurrent_dropout = fDropout_RNN)(lstm_embed_layer) # the output has dimension (None, 12)\n",
    "hidden_layer = Dense(iDim_hidden_nodes_final, activation='relu')(lstm_output)\n",
    "main_output = Dense(Y_train.shape[1], activation='softmax', name='main_output')(hidden_layer) # main output for categories\n",
    "model = Model(inputs=[lstm_input_layer], outputs=[main_output])\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer='adam')\n",
    "model.fit({'lstm_input': X_train}, {'main_output': Y_train}, epochs = n_epoch, batch_size=batch_size, verbose = 2, sample_weight = W_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script: Part 6: Test the final model\n",
    "Using the holdout data (indices `ix_test`), we will estimate hold-out statistics, like the ROC/AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# holdout data for final validation\n",
    "X_test = X[ix_test]; \n",
    "Y_test = Y[ix_test]; \n",
    "\n",
    "# validation on test set\n",
    "pwide =  model.predict(X_test) # predicted probability matrix\n",
    "pvec = pwide.flatten() # vectorize the prediction matrix \n",
    "yvec = Y_test.flatten() # vectorize the one-hot-coding classes\n",
    "\n",
    "# calcualte fpr, tpr, and AUC scores\n",
    "global_fpr, global_tpr, threshold = roc_curve(yvec, pvec)\n",
    "global_roc_auc = auc(global_fpr, global_tpr) # \n",
    "print(\"Global AUC score=%f\" % (global_roc_auc))\n",
    "\n",
    "# plot the ROCurve\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic (out-of-sample)')\n",
    "plt.plot(global_fpr, global_tpr, 'purple', label = 'Global CV-AUC = %0.2f' % global_roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "#plt.savefig('img/global_roc_default_model.png')\n",
    "plt.show() # to plot in your own terminal\n",
    "#Global AUC score=0.949081\n",
    "\n",
    "# calculate AUC scores per category\n",
    "roc_topic = [] # container for per-category AUC scores\n",
    "Y_labels_test = Y_test.argmax(axis=1)\n",
    "for i in range(0,Y.shape[1]):\n",
    "    tmptopic_fpr, tmptopic_tpr, threshold = roc_curve(Y_test[:,i].flatten(), pwide[:,i].flatten())\n",
    "    roc_topic.append(auc(tmptopic_fpr, tmptopic_tpr))\n",
    "\n",
    "# average ROC/AUC score overall categories\n",
    "print(\"Average AUC (holdout) score is %f\" % (sum(roc_topic)/len(roc_topic)))\n",
    "\n",
    "n, bins, patches = plt.hist(roc_topic, 20, facecolor='blue', alpha=0.5)\n",
    "plt.xlabel('AUC scores (per category)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(r'cv-AUC scores per category')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
