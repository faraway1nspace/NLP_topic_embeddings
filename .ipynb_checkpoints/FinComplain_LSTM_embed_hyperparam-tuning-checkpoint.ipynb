{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-Parameter Tuning (default model)\n",
    "This document tunes hyperparameters for use in the \"Category embedding model\" that performs text classification of consumer complaints (using NLP-LSTM mode; see script [here](https://github.com/faraway1nspace/NLP_topic_embeddings/blob/master/FinComplain_LSTM_embedding_model.ipynb)), which is an elaboration of a more generic [LSTM-NLP model](https://github.com/faraway1nspace/NLP_topic_embeddings/blob/master/FinComplain_LSTM_default_model.ipynb); the category embedding is inspired by the famous [instacart model](https://tech.instacart.com/deep-learning-with-emojis-not-math-660ba1ad6cdc). The key hyperparameter is the embedding dimension of the category embeddings.\n",
    "\n",
    "<b>Hyperparameters</b> should be done by minimizing a cross-validation loss. In this script, I use a <b>Thompson-sampler</b> (inspired by a cruide _reinforcement-learning_ / multi-arm bandit algorithm), to stochastically explore the high-dimensional hyperparameter space and find good hyperparameter values. This is an original algorithm and is untested, but it seems to work well.\n",
    "\n",
    "The hyperparameters include:\n",
    "+ the embedding dimension of the category embedding\n",
    "+ the embedding dimension of the word-embedding (like word2vec) \n",
    "+ the size of the corpus used in the word-tokenization (which feeds into the word-embedding). \n",
    "+ the dimensionality of the Long-Short-Term-memory outputs\n",
    "+ the Dropout rate in the LSTM\n",
    "\n",
    "... as well as choosing total number of epochs to run the LSTM.\n",
    "\n",
    "### Category Embeddings\n",
    "The category embedding learns a low-dimensional representation of the >400 different financial complaint Issues in the US Financial Consumer Protection [complaint database](https://www.consumerfinance.gov/data-research/consumer-complaints/). The number of embedding dimensions should be much less than the number of categories, at most 1/2, and probably much less than the sqrt(#[categories]). More dimensions means a potentially better representation but higher variance and redundancy; fewer dimensions means more bias but lower variance: it is a classic trade-off, and should be treated as a hyperparameter. If the embeddings have no affect on predictive performance, then this should be reflected in uncertainty in the hyper-parameter tuning algorithm.\n",
    "\n",
    "During an Insight Data fellowship, I used this technique on a similar proprietary dataset for sentiment analysis, and found it help us make sense of the (growing) number of consumer-product categories.\n",
    "\n",
    "### Function overview\n",
    "+ `run_model` The main function which runs an individual cross-validation run \n",
    "+ `proposal_hyperparam` The main thompson sampler function which proposes new samples from the hyperparameter space.\n",
    "\n",
    "The procedure is straight forward\n",
    "+ get <b>samples</b> from the hyperparameter space\n",
    "+ do <b>3-fold cross-validation</b> to estimate an Expected Loss (aka hold-out loss)\n",
    "+ use the Expected Loss as the 'reward' in a <b>multi-arm bandit learner</b>\n",
    "+ calculate <b>probabilities</b> for each combination of hyperparameters\n",
    "+ use the <b>Thompson-sampler</b>/multi-arm bandit algorithm to draw a new sample from the hyperparameter space\n",
    "+ <b>repeat</b> for about 30 iterations.\n",
    "\n",
    "The multi-arm bandit learner should progressively sample from the hyperparameter space that has a higher-probability of minimizing the Expected Loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions: Data Import & Natural Language Pre-Processing\n",
    "\n",
    "The following functions are some idiosyncratic functions to import and clean the Financial complaint data. For the background of the data source and the models' purpose, please see the [Readme file](https://github.com/faraway1nspace/NLP_topic_embeddings) as well as the [US Consumer Complaint Database](https://www.consumerfinance.gov/data-research/consumer-complaints/) at the US Consumer Financial Protection Bureau website.\n",
    "+ `import_and_clean_data` : reads the complaint data and organizes it for the `keras` LSTM model\n",
    "+ `nlp_preprocess` \" does some basic NLP pre-processing (stemming, removing stop words, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from math import log,exp\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import download as nltk_downloader \n",
    "from keras import backend as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Embedding, LSTM, RepeatVector, concatenate, Dense, Reshape, Flatten\n",
    "from keras.models import Model\n",
    "from scipy.stats import rankdata as rd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "from sklearn.model_selection import KFold # import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# set the working directory\n",
    "#os.chdir(\"...\")\n",
    "\n",
    "# NLP function to replace english contractions\n",
    "def decontracted(phrase):\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "# function to do some basic NLP pre-processing steps: replacing contractions, stemming words, removing stop words\n",
    "def nlp_preprocess(text_column, # column in Panda table with text\n",
    "                   stop_words, # list of English stopwords\n",
    "                   word_clip = 300): # truncate the number of words\n",
    "   # remove contractions\n",
    "   ps = PorterStemmer()  # stemmer    \n",
    "   cTextl = [decontracted(x) for x in text_column.values.tolist()]\n",
    "   # remove double spacing and non-alphanumeric characters\n",
    "   cTextl=[re.sub(' +',' ',re.sub(r'\\W+', ' ', x)) for x in cTextl]\n",
    "   # lower case the words\n",
    "   cTextl = [x.lower() for x in cTextl]\n",
    "   # stop words and stemming\n",
    "   for i in range(0,len(cTextl)):\n",
    "      rawtext = cTextl[i].split(\" \") # splits sentence by spaces\n",
    "      rawtext = rawtext[0:min(word_clip,len(rawtext))] # take only 300 words maximum\n",
    "      # stem and remove stopwords in one line (expensive operation)\n",
    "      newtext = \" \".join(ps.stem(word) for word in rawtext if not word in stop_words)  # loop through words, stem,join\n",
    "      cTextl[i] = newtext\n",
    "   return pd.DataFrame(cTextl)\n",
    "\n",
    "# function: import and pre-process the data (prepare for Keras)\n",
    "def import_and_clean_data(filename, # file name of data to import (either a .csv or a tar.xz file of a .csv)\n",
    "                          col_label = \"Label3\",\n",
    "                          data_dir = \"data/\", # directory\n",
    "                          tmp_dir = \"/tmp/\", # if file is a .tar.xz, where to temporarily extract the data (Windows users need specify differently than /tmp/\n",
    "                          rare_categories_cutoff = 10, # threshold for categories to be included in the training set\n",
    "                          word_clip = 300): # max number of words in text to accept (only first 300 words are retained\n",
    "   # check\n",
    "   if \"tar.xz\" in filename:\n",
    "      print(\"decompressing \" + filename + \" into \"+tmp_dir)\n",
    "      # command for shell\n",
    "      os_system_command = \"tar xf \"+data_dir+filename+\" -C \"+tmp_dir\n",
    "      print(os_system_command)\n",
    "      # run decompression command (for Linux/Mac)\n",
    "      os.system(os_system_command)\n",
    "      newfilename = tmp_dir + filename.split(\".tar.xz\")[0]\n",
    "   else:\n",
    "      print(\"importing csv called \" + filename)\n",
    "      newfilename = data_dir + filename\n",
    "   # read the complaint data \n",
    "   d_raw = pd.read_csv(newfilename, usecols = ['State','Complaint ID','Consumer complaint narrative','Product', 'Sub-product', 'Issue', 'Sub-issue'])\n",
    "   print(\"imported \" + str(d_raw.shape[0]) + \" rows of data\") # notice 191829 rows and 7 columns\n",
    "   # fill NaN with blanks\n",
    "   for col_ in ['Product','Sub-product','Issue']:\n",
    "      d_raw[col_] = d_raw[col_].fillna(\" \") # fill NaN with a character\n",
    "   # factorize the two levels (Product and Product+Issue) to get unique values\n",
    "   d_raw['Label1'] = pd.factorize(d_raw['Product'])[0]\n",
    "   # combine Product + Issues\n",
    "   d_raw['Label3'] = pd.factorize(d_raw['Product'] + d_raw['Sub-product']+d_raw['Issue'])[0] # 570 Categories\n",
    "   # Dictionary: category integers vs. category names\n",
    "   cats = [pd.factorize(d_raw['Product'])[1],  pd.factorize(d_raw['Product'] + d_raw['Sub-product'])[1], pd.factorize(d_raw['Product'] + d_raw['Sub-product']+d_raw['Issue'])[1]]\n",
    "   # truncate the data: only use categories with at least 10 observations\n",
    "   labels_counts = d_raw.groupby([col_label]).size() # counts of Level3 categories \n",
    "   which_labels = np.where(labels_counts>=rare_categories_cutoff)[0] # which categories have at least 'cutoff'\n",
    "   # make new (truncated) dataset\n",
    "   ixSubset = d_raw.Label3.isin(which_labels) # subset integers\n",
    "   # new dataset 'd', as subset of d_raw\n",
    "   d = (d_raw[ixSubset]).copy()\n",
    "   # NLP pre-processing: stopwords removal, stemming, etc.\n",
    "   # get the default English stopwords from nlkt pacakge\n",
    "   from nltk.corpus import stopwords \n",
    "   stop_words = set(stopwords.words('english')) # list of stopwords to remove\n",
    "   # get the stemming object from nltk\n",
    "   ps = PorterStemmer()  # stemmer\n",
    "   # column in data with the Text data (to feed into the LSTM)\n",
    "   col_text = 'Consumer complaint narrative' # name of the column with the text \n",
    "   # NLP: pre-process the text/complaints\n",
    "   print(\"performing NLP pre-processing on column \" + col_text + \" (remove stop words, stemming,...). This may take a while...\")\n",
    "   cText = nlp_preprocess(d[col_text],stop_words, word_clip = word_clip)\n",
    "   print(\"Done NLP pre-processing\")\n",
    "   # process the labels, make into a N-hot-coding matrix \n",
    "   Y = pd.get_dummies(d['Label3'].values) # one-hot coding\n",
    "   # get integers representing each (label3) class (these are the column names)\n",
    "   Ynames_int = Y.columns # notice the confusing mapping of different integers to different integers\n",
    "   # get english issue labels corresponding to each integer value in Ynames_int\n",
    "   Ynames_char = [cats[2][i] for i in Ynames_int] # actual names\n",
    "   # Finally, convert Y into a numpy matrix (not a panda df)\n",
    "   Y = Y.values\n",
    "   # make the input data for the category embedding\n",
    "   uLabels3 = d['Label3'].unique() # unique level 3 labels\n",
    "   nLabels3 = len(uLabels3) # number of unique level 3 labels\n",
    "   X_labels  = np.repeat(np.array([i for i in range(0,nLabels3)],dtype=int).reshape(1,nLabels3),n_obs,axis=0) # input for embedding layer\n",
    "   print(\"returning cleaned text data 'cText' for use in tokenization; 'Y' as an one-hot-coding matrix of categories; and 'cats' a dictionary matches columns in Y to english category names\")\n",
    "   if \"tar.xz\" in filename:\n",
    "      print(\"deleting temporary file \" + filename)\n",
    "      os_system_command = \"rm \"+ newfilename\n",
    "      print(os_system_command)\n",
    "      os.system(os_system_command)\n",
    "      print(\"done pre-processing\")\n",
    "   return cText, Y, X_labels,cats\n",
    "\n",
    "# function to calculate sample_weights for keras argument sample_weight\n",
    "def get_class_weights(Y, # N-hot-coding response matrix\n",
    "                      clip_ = 100000): # maximum weight for rarer cases\n",
    "   weights_total_class_counts = (Y).sum(axis=0)\n",
    "   weights_by_class = (min(weights_total_class_counts)/weights_total_class_counts) # weight by the rarest case\n",
    "   Y_int = np.argmax(Y,axis=1)\n",
    "   vWeights_raw = np.array([weights_by_class[i] for i in Y_int], dtype=float)\n",
    "   vWeights = np.clip(vWeights_raw * (Y.shape[0]/sum(vWeights_raw)),0,clip_)\n",
    "   return vWeights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions: Hyperparameter Sampling\n",
    "\n",
    "With 4 hyperparameters and (at least) 3 discrete values each, I explored 81 possible _combinations_ of hyperparameter (for an academic or production-level project, you'd likely want more fine-grained hyperparameter values). The goal is to find the best combination of hyperparameter values, based on a cross-validation loss (Expected Loss). It is very inefficient to do cross-validation on EVERY combination. Instead, we'll do a <b>stochastic search</b> across the space of hyperparameter combinations, arriving at the best combination much earlier than running all 81 combinations.\n",
    "\n",
    "#### Thompson sampling:\n",
    "Stochastic exploration of the hyperparameter space requires some way to calculate _probabilities_ for each combination of hyperparameters. I use a simple principle from reinforcement learning called <b>Thompson sampling</b>: the more _uncertain_ a potential action/reward is, the more _likely_ we should try it, and thus reduce our uncertainty quickly find the most rewarding action. Here _action_ means picking a combination of hyperparameters to estimate the Expected Loss; and _reward_ is finding the lowest Expected Loss.\n",
    "\n",
    "#### Model uncertainty as a Dirichlet-Multinomial process\n",
    "The key innovation I present is my method to estimate the probabilities for each hyperparameter combination: this will involve an ensemble of penalized-regressors (`Ridge` and `DecisionTreeRegressor` from `sklearn`). The regressors will estimate the Expected Loss of each combination using the hyperparameter variables as predictor variables. The _frequency counts_ of each hyperparameter-combination being the best model (lowest predicted loss) is converted into a probability via the <b>Dirichlet-Multinomial</b> process. \n",
    "\n",
    "There are a bunch of functions, the main ones pertaining to the hyperparameters are:\n",
    " + `make_hyperparameters_combos` : takes a dictionary of hyperparameters values and makes a grid of all possible combinations\n",
    " + `optimal_order_of_hyperparameter_runs` : proposes an optimal sequence in which to run the different combination of hyperparameter values. This will find high-contrasting parameter-combinations, so that the space of hyperparameter values is quickly explored deterministically, prior to evoking the multi-arm bandit algorithm\n",
    " + `proposal_hyperparam`: main Thompson sampler; proposes new combinations of hyperparameters to run in cross-validation. It switches between a deterministic algorithm and a stochastic algorithm after a preset number of iterations (`toggle_learner`). There are other hyper-hyper-parameters that govern the learning behaviour of the multi-arm-bandit process (`ridge_alpha`, `max_depth`,`multinomial_prior`) and should be left as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# internal function for cross-validation: creates all combinations of hyperparameters\n",
    "def make_hyperparameters_combos(hyper_parameters):\n",
    "   # hyperparmeters' values\n",
    "   hyp_args = [x[1] for x in hyper_parameters.items()]\n",
    "   # names of hyperparameters\n",
    "   hyp_names = [x[0] for x in hyper_parameters.items()]\n",
    "   # all combos of hyperparameters, as a tensor\n",
    "   hyp_args_grid = np.meshgrid(*hyp_args)\n",
    "   # dimensions of the hyperparameters\n",
    "   hyp_args_dimensions = list(hyp_args_grid[0].shape)\n",
    "   # total number of hyperparameter combos\n",
    "   total_combos = round(exp(sum(map(log,hyp_args_dimensions))))\n",
    "   # reshape into a [n_combos,parameters]\n",
    "   hyp_grid = (np.array(hyp_args_grid).reshape(len(hyp_args_grid),total_combos)).T\n",
    "   # convert grid to data.frame\n",
    "   hyp_pd = pd.DataFrame(hyp_grid, columns = hyp_names)\n",
    "   # also make a companion sequence\n",
    "   hyp_seq = [[j for j in range(0,len(x[1]))] for x in hyper_parameters.items()]\n",
    "   hyp_seq_grid = (np.array(np.meshgrid(*hyp_seq)).reshape(len(hyp_args_grid),total_combos)).T\n",
    "   # make an empty panda data.frame to fill with results\n",
    "   empty_res = pd.DataFrame({\"cv_loss\": [0 for i in range(0,hyp_pd.shape[0])], \"best_epoch\": [0 for i in range(0,hyp_pd.shape[0])]})\n",
    "   return hyp_pd, hyp_seq_grid, empty_res\n",
    "\n",
    "# internal function for cross-validation: optimal order of running different hyperparameter scenarios: calculates a 'scenario distance' by finding scenarios that are maximally contrasting with each other\n",
    "def optimal_order_of_hyperparameter_runs(hyp_seq_grid):\n",
    "   scenario_dist = -2 * np.dot(hyp_seq_grid, hyp_seq_grid.T) + np.sum(hyp_seq_grid**2, axis=1) + np.sum(hyp_seq_grid**2, axis=1)[:, np.newaxis]\n",
    "   scenario_rows = [0] # start with row 1\n",
    "   for i in range(0,hyp_seq_grid.shape[0]):\n",
    "      cur_dist = -2 * np.dot(hyp_seq_grid, hyp_seq_grid[scenario_rows].T) + np.sum(hyp_seq_grid[scenario_rows]**2, axis=1) + np.sum(hyp_seq_grid**2, axis=1)[:, np.newaxis]\n",
    "      elem_rank_by_distance = rd(-(((cur_dist**2)).sum(axis=1))**(0.5)).argsort()\n",
    "      pos_elem = [x for x in elem_rank_by_distance if x not in scenario_rows]\n",
    "      if len(pos_elem)>0:\n",
    "         scenario_rows.append(pos_elem[0])\n",
    "   return scenario_rows\n",
    "\n",
    "# internal function for cross-validation: make validation sets and test sets\n",
    "def make_cv_weights(n_obs, fHoldoutProportion = 0.5, kfold = 3, seed = 1000):\n",
    "   # trainging set and test set\n",
    "   ix_train, ix_test = train_test_split([i for i in range(0,n_obs)], test_size = fHoldoutProportion, random_state = seed)\n",
    "   # divide the training data into cross-validation sets\n",
    "   cv_splitter = KFold(n_splits = kfold)\n",
    "   cv_sets = []\n",
    "   for ix_insample, ix_validation in cv_splitter.split(ix_train):\n",
    "      cv_sets.append([ix_insample, ix_validation])\n",
    "   return ix_train, ix_test, cv_sets\n",
    "\n",
    "# internal function for cross-validation: propose hyperparameters, by two methods:\n",
    "# ... i) sequential (just loops through combinations)\n",
    "# ... ii) thompson sampling / multi-arm bandit reinforcement learner (ridge regression & decision trees to try to predict what might be best hyperparameters)\n",
    "def proposal_hyperparam(run_number, # iteration number for the algorithm\n",
    "                        cv_results, # current results of the cv-loss (panda frame)\n",
    "                        hyp_pd, # output from \"make_hyperparameters_combos\" function\n",
    "                        hyp_optimal, # output from \"make_hyperparameters_combos\" function\n",
    "                        toggle_learner = 10, # when to switch to mult-arm bandit learning\n",
    "                        ridge_alpha = 7, # ridge regression learner: shrinkage parameter\n",
    "                        max_depth = 2, # tree-learner maximum tree depth\n",
    "                        multinomial_prior = 1): # diffuse prior on the model space for the multi-arm bandit \n",
    "   n_models = len(hyp_optimal)\n",
    "   if (run_number < toggle_learner) | (run_number > (hyp_pd.shape[0]-1)):\n",
    "      # just sequentially loop through hyperparameter combos\n",
    "      print(\"next hyperparameter: estimated by maximum parameter contrast\")\n",
    "      return_hypIx = hyp_optimal[run_number]\n",
    "      return_hyperparameters = hyp_pd.iloc[return_hypIx]\n",
    "   else:\n",
    "      print(\"next hyperparameter: Thompson sampling of hyperparameters based on a multi-arm bandit learner\") \n",
    "        # find which hyperparam/combos are already completed (used for estimation)\n",
    "      which_done = np.where(cv_results['best_epoch'].values >0)\n",
    "      # find which hyperparam/combos are not yet run (used for predicting their loss)\n",
    "      which_notdone = np.where(cv_results['best_epoch'].values == 0)\n",
    "      # train a ridge regression \n",
    "      learner1 = Ridge(alpha=ridge_alpha, copy_X = True,normalize=True)\n",
    "      # train a decision tree\n",
    "      learner2 = DecisionTreeRegressor(max_depth = max_depth)\n",
    "      # multi-arm bandit: use leave-one-out estimation to get frequency counts that each combo is the best\n",
    "      counts_best_loss = np.zeros(len(which_notdone[0])) + multinomial_prior # notice shrinkage parameter multinomial_prior=1\n",
    "      # loop through each completed combo and drop it (leave-one-out subsampling)\n",
    "      for j in range(0,len(which_done[0])):\n",
    "         # drop the j'th combo (leave-one-out)\n",
    "         loo = which_done[0][np.arange(len(which_done[0]))!=j]\n",
    "         # fit learners\n",
    "         lr1 = learner1.fit(X = hyp_pd.values[loo], y = cv_results['cv_loss'].values[loo]) \n",
    "         lr2 = learner2.fit(X = hyp_pd.values[loo], y = cv_results['cv_loss'].values[loo]) \n",
    "         # predict which will be the lowest loss         \n",
    "         pred_loss1 = lr1.predict(hyp_pd.values[which_notdone]) # expected loss from the learner\n",
    "         pred_loss2 = lr2.predict(hyp_pd.values[which_notdone]) # expected loss from the learner         \n",
    "         # increment number of time's each hypparam/combo is predicted to be the best\n",
    "         counts_best_loss += 0.5*(pred_loss1 == min(pred_loss1))\n",
    "         counts_best_loss += 0.5*(pred_loss2 == min(pred_loss2))\n",
    "      # convert selection frequency of each combo into a probability (Dirichlet)\n",
    "      thompson_sampling_prob = np.random.dirichlet(counts_best_loss)\n",
    "      # Thompson sampler: random sample from the probabilities (Multinomial)\n",
    "      return_hypIx = np.random.choice(which_notdone[0],p=thompson_sampling_prob)\n",
    "      # get the index of the hyperparameters for the best expectd loss\n",
    "      # hyperparameters for the best expectd loss\n",
    "      return_hyperparameters = hyp_pd.iloc[return_hypIx]\n",
    "   return return_hypIx, return_hyperparameters \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions: K-fold Cross-Validation\n",
    "The following functions support the 3-fold cross validation. The are fairly self-explanatory. The main functions are:\n",
    "+ `make_cv_weights` : splits the data into two indices for training & testing. Among the training indices (`ix_train`), it further divides the data into mutually exclusive k-fold cross-validation sets (`cv_sets`). You tune the hyperparameters with the subsets in `cv_sets`; you train a final (tuned) model on the `ix_train`, and you validate the final model on the hold-out set `ix_test`.\n",
    "+ `run_model` : runs an LSTM model using the `keras` API; \n",
    "+ `run_cv_model`: calls `run_model` for one CV-iteration; ONLY returns the validation loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# internal function for cross-validation: make validation sets and test sets\n",
    "def make_cv_weights(n_obs, fHoldoutProportion = 0.5, kfold = 3, seed = 1000):\n",
    "   # trainging set and test set\n",
    "   ix_train, ix_test = train_test_split([i for i in range(0,n_obs)], test_size = fHoldoutProportion, random_state = seed)\n",
    "   # divide the training data into cross-validation sets\n",
    "   cv_splitter = KFold(n_splits = kfold)\n",
    "   cv_sets = []\n",
    "   for ix_insample, ix_validation in cv_splitter.split(ix_train):\n",
    "      cv_sets.append([ix_insample, ix_validation])\n",
    "   return ix_train, ix_test, cv_sets\n",
    "\n",
    "# function: LSTM model, using Keras functional API\n",
    "# use the Keras API to build the model\n",
    "# Main LSTM model. Two inputs: X=word features (tokenized); X_labels=matrix of categories [1,2,3,...,nLabels]\n",
    "# ... the output is Y: one-hot-coding of labels for each row of X,X_labels.\n",
    "# Notice the `RepeatVector` which merges the output from the LSTM with the output from the Category embedding\n",
    "def run_model(X, # tokenized text\n",
    "              X_labels, # categories vector [1,2,3,...]\n",
    "              Y, # labels (one-hot-coding)\n",
    "              W, # sample weights\n",
    "              X_val, # out-of-sample validation: X\n",
    "              X_labels_val, # # out-of-sample validation: X_labels\n",
    "              Y_val, # # out-of-sample validation: Y\n",
    "              max_tokens, # number of words/tokens used to tokenize sentences\n",
    "              n_epoch = 30, # hyperparameter\n",
    "              batch_size = 64, # hyperparameter\n",
    "              dim_embed_lstm= 200, # word embedding dimension\n",
    "              dim_out_lstm =100, # dimension of LSTM output\n",
    "              fDropout_RNN=0.1, # regularization: recurrent_dropout for LSTM\n",
    "              fDropout=0.33, # regularization: input dropout for LSTM\n",
    "              dim_embed_categ=5, # embedding dimension of the categories\n",
    "              dim_hidden_nodes_final =100): # number of hidden nodes in final Dense layer\n",
    "   nLabels = X_labels.shape[1]\n",
    "   # main input: the word features (tokenized) \n",
    "   lstm_input_layer = Input(shape=(X.shape[1],), dtype='int32', name='lstm_input',) # lstm input\n",
    "   # word embedding (like word2vec)\n",
    "   lstm_embed_layer = Embedding(input_dim=max_tokens, output_dim=dim_embed_lstm, input_length=X.shape[1])(lstm_input_layer)\n",
    "   lstm_output = LSTM(dim_out_lstm, dropout = fDropout, recurrent_dropout = fDropout_RNN)(lstm_embed_layer)\n",
    "   # reshape the LSTM output to concatenate with the category embedding\n",
    "   lstm_reshape = RepeatVector(nLabels)(lstm_output)  \n",
    "   label3_input_layer = Input(shape=(X_labels.shape[1],), dtype='int32', name='label3_input') # input the topics\n",
    "   label3_embed_layer = Embedding(input_dim=nLabels, output_dim = dim_embed_categ, input_length=X_labels.shape[1])(label3_input_layer) # topic embedding: should have dim: None,7,embed_dim\n",
    "   # merge the LSTM output with the category embedding\n",
    "   x = concatenate([lstm_reshape,label3_embed_layer],axis=2) # \n",
    "   # final hidden layer\n",
    "   hidden_layer = Dense(dim_hidden_nodes_final, activation='relu')(x)\n",
    "   final_layer = Dense(1, activation='sigmoid')(hidden_layer) # main output for categories\n",
    "   # reshape the output so that it is multinomial in [n_obs, n_categories]\n",
    "   final_layer2 = Flatten()(final_layer) # dimension: [n_obs, n_categories]\n",
    "   main_output = Dense(Y.shape[1], activation='softmax',name = 'main_output')(final_layer2) # main output for categories \n",
    "   model = Model(inputs=[lstm_input_layer, label3_input_layer], outputs=[main_output])\n",
    "   model.compile(loss = \"categorical_crossentropy\", optimizer='adam')\n",
    "   print(model.summary()) # \n",
    "   history = model.fit({'lstm_input': X, 'label3_input': X_labels }, {'main_output': Y}, epochs = n_epoch, batch_size=batch_size, verbose = 0, sample_weight = W, validation_data=([X_val, X_labels_val], Y_val))\n",
    "   return model, history\n",
    "\n",
    "# main function: run individual cv-run; ONLY returns the validation loss (deletes model)\n",
    "def run_cv_model(cv_insample_indices,cv_validation_indices, Y, cText, X_labels, vWeights, hyperparam, n_epoch = 7, fDropout_RNN = 0.1, batch_size = 64):\n",
    "   n_obs = Y.shape[0] # number of observations/rows\n",
    "   # get hyperparameters\n",
    "   iMaxTokens = int(hyperparam['max_tokens']) # maximum number of words in corpus for embedding\n",
    "   iDimEmbedLSTM = int(hyperparam['DimEmbedLSTM']) # embedding dimensions for words\n",
    "   iDimOutLSTM = int(hyperparam['DimOutLSTM']) # LSTM output dimension\n",
    "   fDropout = hyperparam['Dropout'] # LSTM input dropout regularization\n",
    "   iDimEmbedCategory = hyperparam['DimEmbedCategory']\n",
    "   iDim_hidden_nodes_final = (np.linspace(iDimOutLSTM,Y.shape[1]).round().astype(int))[1] # number of    \n",
    "   # tokenize the text\n",
    "   tokenizer = Tokenizer(num_words=iMaxTokens, split=' ')\n",
    "   tokenizer.fit_on_texts(\"STARTCODON \" + cText[0])\n",
    "   # text data: insample and validation\n",
    "   X = pad_sequences(tokenizer.texts_to_sequences((\"STARTCODON \" + cText[0]).values)) # tokenize and pad with zeros\n",
    "   X_insample = X[cv_insample_indices] \n",
    "   X_val = X[cv_validation_indices]\n",
    "   # label data (N-hot-coding): insample and validation\n",
    "   Y_insample = Y[cv_insample_indices]\n",
    "   Y_val = Y[cv_validation_indices]\n",
    "   # labels for category embedding\n",
    "   X_labels_insample = X_labels[cv_insample_indices]\n",
    "   X_labels_val = X_labels[cv_validation_indices]\n",
    "   # training weights\n",
    "   W_insample = vWeights[cv_insample_indices]\n",
    "   # fit the model\n",
    "   model, history = run_model(X_insample, X_labels_insample, Y_insample, W_insample, \n",
    "                              X_val,X_labels_val,Y_val, \n",
    "                              iMaxTokens, \n",
    "                              n_epoch, \n",
    "                              batch_size, \n",
    "                              iDimEmbedLSTM, \n",
    "                              iDimOutLSTM, \n",
    "                              fDropout_RNN, \n",
    "                              fDropout, \n",
    "                              iDimEmbedCategory, \n",
    "                              iDim_hidden_nodes_final)\n",
    "   val_loss = history.history['val_loss']\n",
    "   del model\n",
    "   del history\n",
    "   return val_loss\n",
    "\n",
    "# internal function for cross-validation: get the best epoch over k-fold validation\n",
    "def get_best_epoch(cv_losses):\n",
    "      expected_loss_per_epoch = (np.array(cv_losses).T).mean(axis=1)\n",
    "      best_epoch = expected_loss_per_epoch.argmin() + 1\n",
    "      return best_epoch, expected_loss_per_epoch[best_epoch-1]\n",
    "\n",
    "# internal function for cross-validation: get best hyperparameter combinations from CV-loss results\n",
    "def get_best_hyperparameters(cv_results, hyperparameter_grid):\n",
    "   which_done = np.where(cv_results['best_epoch'].values!=0)\n",
    "   which_best = which_done[0][cv_results['cv_loss'].values[which_done[0]].argmin()]\n",
    "   best_epoch = cv_results['best_epoch'][which_best]\n",
    "   return hyperparameter_grid.iloc[which_best], best_epoch \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script: Part 1: import and process data\n",
    "\n",
    "+ NLP the text data (`cText`). \n",
    "+ Convert the categories/labels into one-hot-coding (`Y`)\n",
    "+ Make the category embedding (`X_labels3`).\n",
    "+ Make sample weights (`vWeights`) for the keras `sample_weight` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decompressing complaints-2018-09-30_17_53.csv.tar.xz into /tmp/\n",
      "tar xf data/complaints-2018-09-30_17_53.csv.tar.xz -C /tmp/\n",
      "imported 191828 rows of data\n",
      "performing NLP pre-processing on column Consumer complaint narrative (remove stop words, stemming,...). This may take a while...\n",
      "Done NLP pre-processing\n",
      "returning cleaned text data 'cText' for use in tokenization; 'Y' as an one-hot-coding matrix of categories; and 'cats' a dictionary matches columns in Y to english category names\n",
      "deleting temporary file complaints-2018-09-30_17_53.csv.tar.xz\n",
      "rm /tmp/complaints-2018-09-30_17_53.csv\n",
      "done pre-processing\n",
      "number of observations:191193\n",
      "number of categories:425\n"
     ]
    }
   ],
   "source": [
    "# import and clean the data.\n",
    "# Returns 'cText' (the NLP-preprocessed data) and 'cats' (the dictionary of categories/labels for classification')\n",
    "cText, Y, X_labels3, cats = import_and_clean_data(filename = \"complaints-2018-09-30_17_53.csv.tar.xz\", # filename to import the data,\n",
    "    #filename = \"complaints-2018-09-30_17_53.csv\", # filename to import the data,     \n",
    "                                    col_label = \"Label3\", # label to work with for classification/learning\n",
    "                                    data_dir = \"data/\", # directory\n",
    "                                    tmp_dir = \"/tmp/\", # if file is a .tar.xz, where to temporarily extract the data (Windows users need specify differently than /tmp/\n",
    "                                    rare_categories_cutoff = 10, # threshold for categories to be included in the training set\n",
    "                                    word_clip = 300) # max number of words in text to accept (only first 300 words are retained\n",
    "\n",
    "n_obs = Y.shape[0]\n",
    "nLabels3 = X_labels3.shape[1]\n",
    "print(\"number of observations:\" + str(n_obs))\n",
    "print(\"number of categories:\" + str(nLabels3))\n",
    "      \n",
    "# sample weights to correct for unbalanced categories / labels\n",
    "vWeights = get_class_weights(Y,5) # weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script: Part 2: define the hyperparameters and set-up cross-validation\n",
    "### validation sets\n",
    "+ `kfold` : used for k-fold cross-validation; I use 3-fold validation\n",
    "+ `fHoldoutProportion` : used to define the proportion of data not-used for training, but for validating the final model.\n",
    "\n",
    "### hyperparameter space\n",
    "+ `max_tokens`: the size of the corpus used in the word-tokenization (which feeds into the word-embedding).\n",
    "+ `DimEmbedLSTM`:the embedding dimension of the word-embedding (like word2vec) \n",
    "+ `DimOutLSTM` : the dimensionality of the Long-Short-Term-memory outputs\n",
    "+ `Dropout` regularization parameter: dropout rate in the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = 3 # k-fold cross-validation\n",
    "fHoldoutProportion = 0.8 # amount of data to leave aside for (final) hold-out validation\n",
    "\n",
    "# hyperparmeters' and their values\n",
    "hyper_parameters = {'max_tokens': [1500,2000,3000], # number of wor tokens\n",
    "                    'DimEmbedLSTM':[97,194,291], # word embedding dimension\n",
    "                    'DimOutLSTM':[100,150,300], # LSTM output dimension\n",
    "                    'Dropout':[0.20,0.33,0.5], # dropout proportion for LSTM\n",
    "                    'DimEmbedCategory':[2,4,8,12,16,20]} # category embedding\n",
    "\n",
    "# hyperparameters: make all possible combinations of \n",
    "hyp_grid, ix_hyp_grid, cv_results = make_hyperparameters_combos(hyper_parameters)\n",
    "optimal_hyp_order = optimal_order_of_hyperparameter_runs(ix_hyp_grid)\n",
    "\n",
    "# training set, testing sets, and cross-validation sets: indicies to divide the data\n",
    "ix_train,ix_test,cv_sets = make_cv_weights(n_obs, fHoldoutProportion = fHoldoutProportion, kfold = kfold, seed = 1000)\n",
    "\n",
    "n_epoch = 40 # number of epochs\n",
    "batch_size = 200 # batchsize\n",
    "pause_seconds = 30 # pause between CV-iterations\n",
    "\n",
    "# big loop to run through hyperparameters\n",
    "max_hyperparameter_runs = min(20, len(optimal_hyp_order)) # maximum number of iterations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script: Part 3: Explore the hyperparameter space (cross-validation)\n",
    "This is the main part of the exercise, consisting of two massive loops. The outer loop iterates through different hyperparameter values, and the inner loop iterates through the K-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next hyperparameter: estimated by maximum parameter contrast\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-c8ceb1b65d2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0micv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mlcv_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_sets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mk_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_cv_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlcv_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlcv_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcText\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_labels3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvWeights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_hyper_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfDropout_RNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m    \u001b[0;31m# return best mean(validation_loss) at which epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#   best_epoch, k_expected_loss = get_best_epoch(k_loss) # get best epoch and expected loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-483f7979f255>\u001b[0m in \u001b[0;36mrun_cv_model\u001b[0;34m(cv_insample_indices, cv_validation_indices, Y, cText, X_labels, vWeights, hyperparam, n_epoch, fDropout_RNN, batch_size)\u001b[0m\n\u001b[1;32m     91\u001b[0m                               \u001b[0mfDropout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                               \u001b[0miDimEmbedCategory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                               iDim_hidden_nodes_final)\n\u001b[0m\u001b[1;32m     94\u001b[0m    \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m    \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-483f7979f255>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(X, X_labels, Y, W, X_val, X_labels_val, Y_val, max_tokens, n_epoch, batch_size, dim_embed_lstm, dim_out_lstm, fDropout_RNN, fDropout, dim_embed_categ, dim_hidden_nodes_final)\u001b[0m\n\u001b[1;32m     40\u001b[0m    \u001b[0mlstm_reshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRepeatVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnLabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m    \u001b[0mlabel3_input_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'label3_input'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# input the topics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m    \u001b[0mlabel3_embed_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnLabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdim_embed_categ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel3_input_layer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# topic embedding: should have dim: None,7,embed_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m    \u001b[0;31m# merge the LSTM output with the category embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m    \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlstm_reshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel3_embed_layer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# is this axis 1 or 2??\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m                                          \u001b[0;34m'You can build it manually via: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m                                          '`layer.build(batch_input_shape)`')\n\u001b[0;32m--> 431\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/layers/embeddings.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings_regularizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             dtype=self.dtype)\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         weight = K.variable(initializer(shape),\n\u001b[0m\u001b[1;32m    250\u001b[0m                             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m                             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/initializers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, shape, dtype)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         return K.random_uniform(shape, self.minval, self.maxval,\n\u001b[0;32m--> 112\u001b[0;31m                                 dtype=dtype, seed=self.seed)\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(shape, minval, maxval, dtype, seed)\u001b[0m\n\u001b[1;32m   4047\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4048\u001b[0m     return tf.random_uniform(shape, minval=minval, maxval=maxval,\n\u001b[0;32m-> 4049\u001b[0;31m                              dtype=dtype, seed=seed)\n\u001b[0m\u001b[1;32m   4050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/random_ops.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(shape, minval, maxval, dtype, seed, name)\u001b[0m\n\u001b[1;32m    240\u001b[0m           shape, minval, maxval, seed=seed1, seed2=seed2, name=name)\n\u001b[1;32m    241\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m       \u001b[0mrnd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_random_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_uniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnd\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmaxval\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mminval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_random_ops.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(shape, dtype, seed, seed2, name)\u001b[0m\n\u001b[1;32m    731\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m    732\u001b[0m         \u001b[0;34m\"RandomUniform\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m    734\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    607\u001b[0m               _SatisfiesTypeConstraint(base_type,\n\u001b[1;32m    608\u001b[0m                                        \u001b[0m_Attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m                                        param_name=input_name)\n\u001b[0m\u001b[1;32m    610\u001b[0m             \u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0minferred_from\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_SatisfiesTypeConstraint\u001b[0;34m(dtype, attr_def, param_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m           \u001b[0;34m\"allowed values: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m           (param_name, dtypes.as_dtype(dtype).name,\n\u001b[0;32m---> 60\u001b[0;31m            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Value passed to parameter 'shape' has DataType float32 not in list of allowed values: int32, int64"
     ]
    }
   ],
   "source": [
    "i = 0 # iterator, outer loop\n",
    "#print(\"running k-fold hyperparameter estimation algorithm for \" + str(max_hyperparameter_runs) + \"iterations\")\n",
    "#while i < max_hyperparameter_runs:\n",
    "#   print(\"hyperparameter tuning, iteration: \" + str(i))\n",
    "   # proposal new hyperparameters from the space of combinations\n",
    "ixHyper, current_hyper_param = proposal_hyperparam(i, cv_results, hyp_pd = hyp_grid, hyp_optimal = optimal_hyp_order, toggle_learner = 10, ridge_alpha = 7)\n",
    "current_hyper_param.values\n",
    "   # do cross-validation\n",
    "k_loss = [] # validation loss results\n",
    "   # inner loop, cross-validation\n",
    "#   for icv, lcv_set in enumerate(cv_sets):\n",
    "icv = 0\n",
    "lcv_set = cv_sets[0]\n",
    "k_loss.append(run_cv_model(lcv_set[0],lcv_set[1], Y, cText, X_labels3, vWeights, current_hyper_param, n_epoch = n_epoch, fDropout_RNN = 0.1, batch_size = batch_size))\n",
    "   # return best mean(validation_loss) at which epoch \n",
    "#   best_epoch, k_expected_loss = get_best_epoch(k_loss) # get best epoch and expected loss\n",
    "#   cv_results['best_epoch'].iloc[ixHyper] = best_epoch\n",
    "#   cv_results['cv_loss'].iloc[ixHyper] = k_expected_loss\n",
    "#   print(\"done iteration \"+ str(i)+ \". Sleeping for \" + str(pause_seconds) +\" seconds.\")\n",
    "#   time.sleep(pause_seconds)\n",
    "   # repeat\n",
    "#   i += 1\n",
    "\n",
    "print(\"done k-fold hyperparameter estimation: hit \" + str(max_hyperparameter_runs) + \" iterations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script: Part 4: Get best hyperparameters (tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the CV results: get best hyperparameters\n",
    "best_hyperparameters, n_epoch = get_best_hyperparameters(cv_results, hyp_grid)\n",
    "cv_results = pd.concat([pd.DataFrame(hyp_grid),cv_results],axis=1)\n",
    "cv_results = cv_results[cv_results['best_epoch']!=0]\n",
    "cv_results.sort_values(by=['cv_loss'])\n",
    "print(cv_results) # print the results of cross-validation\n",
    "\n",
    "# Final best hyperparameters\n",
    "iMaxTokens = int(best_hyperparameters['max_tokens'])\n",
    "iDimEmbedLSTM = int(best_hyperparameters['DimEmbedLSTM'])\n",
    "iDimOutLSTM = int(best_hyperparameters['DimOutLSTM'])\n",
    "fDropout = best_hyperparameters['Dropout']\n",
    "iDim_hidden_nodes_final = (np.linspace(iDimOutLSTM,Y.shape[1]).round().astype(int))[1] # number of\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script: Part 5: Run the final 'tuned' model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit final model\n",
    "tokenizer = Tokenizer(num_words=iMaxTokens, split=' ')\n",
    "tokenizer.fit_on_texts(\"STARTCODON \" + cText[0])\n",
    "# text data: insample and validation\n",
    "X = pad_sequences(tokenizer.texts_to_sequences((\"STARTCODON \" + cText[0]).values)) # tokenize and pad with zeros\n",
    "X_train = X[ix_train];\n",
    "Y_train = Y[ix_train]\n",
    "W_train = vWeights[ix_train]\n",
    "# final model\n",
    "lstm_input_layer = Input(shape=(X_train.shape[1],), dtype='int32', name='lstm_input',) # lstm input\n",
    "lstm_embed_layer = Embedding(input_dim=iMaxTokens, output_dim=iDimEmbedLSTM, input_length=X_train.shape[1])(lstm_input_layer) # input_dim = vocab size,\n",
    "lstm_output = LSTM(iDimOutLSTM, dropout = fDropout, recurrent_dropout = fDropout_RNN)(lstm_embed_layer) # the output has dimension (None, 12)\n",
    "hidden_layer = Dense(iDim_hidden_nodes_final, activation='relu')(lstm_output)\n",
    "main_output = Dense(Y_train.shape[1], activation='softmax', name='main_output')(hidden_layer) # main output for categories\n",
    "model = Model(inputs=[lstm_input_layer], outputs=[main_output])\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer='adam')\n",
    "model.fit({'lstm_input': X_train}, {'main_output': Y_train}, epochs = n_epoch, batch_size=batch_size, verbose = 2, sample_weight = W_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script: Part 6: Test the final model\n",
    "Using the holdout data (indices `ix_test`), we will estimate hold-out statistics, like the ROC/AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# holdout data for final validation\n",
    "X_test = X[ix_test]; \n",
    "Y_test = Y[ix_test]; \n",
    "\n",
    "# validation on test set\n",
    "pwide =  model.predict(X_test) # predicted probability matrix\n",
    "pvec = pwide.flatten() # vectorize the prediction matrix \n",
    "yvec = Y_test.flatten() # vectorize the one-hot-coding classes\n",
    "\n",
    "# calcualte fpr, tpr, and AUC scores\n",
    "global_fpr, global_tpr, threshold = roc_curve(yvec, pvec)\n",
    "global_roc_auc = auc(global_fpr, global_tpr) # \n",
    "print(\"Global AUC score=%f\" % (global_roc_auc))\n",
    "\n",
    "# plot the ROCurve\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic (out-of-sample)')\n",
    "plt.plot(global_fpr, global_tpr, 'purple', label = 'Global CV-AUC = %0.2f' % global_roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "#plt.savefig('img/global_roc_default_model.png')\n",
    "plt.show() # to plot in your own terminal\n",
    "#Global AUC score=0.949081\n",
    "\n",
    "# calculate AUC scores per category\n",
    "roc_topic = [] # container for per-category AUC scores\n",
    "Y_labels_test = Y_test.argmax(axis=1)\n",
    "for i in range(0,Y.shape[1]):\n",
    "    tmptopic_fpr, tmptopic_tpr, threshold = roc_curve(Y_test[:,i].flatten(), pwide[:,i].flatten())\n",
    "    roc_topic.append(auc(tmptopic_fpr, tmptopic_tpr))\n",
    "\n",
    "# average ROC/AUC score overall categories\n",
    "print(\"Average AUC (holdout) score is %f\" % (sum(roc_topic)/len(roc_topic)))\n",
    "\n",
    "n, bins, patches = plt.hist(roc_topic, 20, facecolor='blue', alpha=0.5)\n",
    "plt.xlabel('AUC scores (per category)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(r'cv-AUC scores per category')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
