{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-Parameter Tuning (default model)\n",
    "This document perfoms cross-validation tuning of hyperparameters, for use in the \"Default\" model performing text classification of consumer complaints (using NLP-LSTM mode; see script [here](https://github.com/faraway1nspace/NLP_topic_embeddings/blob/master/FinComplain_LSTM_default_model.ipynb)).\n",
    "\n",
    "<b>Hyperparameters</b> should be done by minimizing a cross-validation loss. In this script, I use a <b>Thompson-sampler</b> (inspired by a cruide _reinforcement-learning_ / multi-arm bandit algorithm), to stochastically explore the high-dimensional hyperparameter space and find good hyperparameter values. This is an original algorithm and is untested, but seems to work well.\n",
    "\n",
    "The hyperparameters include:\n",
    "+ the embedding dimension of the word-embedding (like word2vec) \n",
    "+ the size of the corpus used in the word-tokenization (which feeds into the word-embedding). \n",
    "+ the dimensionality of the Long-Short-Term-memory outputs\n",
    "+ the Dropout rate in the LSTM\n",
    "\n",
    "... as well as choosing total number of epochs to run the LSTM.\n",
    "\n",
    "### Function overview\n",
    "+ `run_model` The main function which runs an individual cross-validation run \n",
    "+ `proposal_hyperparam` The main thompson sampler function which proposes new samples from the hyperparameter space.\n",
    "\n",
    "The procedure is straight forward\n",
    "+ get <b>samples</b> from the hyperparameter space\n",
    "+ do <b>3-fold cross-validation</b> to estimate an Expected Loss (aka hold-out loss)\n",
    "+ use the Expected Loss as the 'reward' in a <b>multi-arm bandit learner</b>\n",
    "+ calculate <b>probabilities</b> for each combination of hyperparameters\n",
    "+ use the <b>Thompson-sampler</b>/multi-arm bandit algorithm to draw a new sample from the hyperparameter space\n",
    "+ <b>repeat</b> for about 30 iterations.\n",
    "\n",
    "The multi-arm bandit learner should progressively sample from the hyperparameter space that has a higher-probability of minimizing the Expected Loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions: Data Import & Natural Language Pre-Processing\n",
    "\n",
    "The following functions are some idiosyncratic functions to import and clean the Financial complaint data. For the background of the data source and the models' purpose, please see the [Readme file](https://github.com/faraway1nspace/NLP_topic_embeddings) as well as the [US Consumer Complaint Database](https://www.consumerfinance.gov/data-research/consumer-complaints/) at the US Consumer Financial Protection Bureau website.\n",
    "+ `import_and_clean_data` : reads the complaint data and organizes it for the `keras` LSTM model\n",
    "+ `nlp_preprocess` \" does some basic NLP pre-processing (stemming, removing stop words, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from math import log,exp\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import download as nltk_downloader \n",
    "from keras import backend as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Embedding, LSTM, RepeatVector, concatenate, Dense, Reshape, Flatten\n",
    "from keras.models import Model\n",
    "from scipy.stats import rankdata as rd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "from sklearn.model_selection import KFold # import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# set the working directory\n",
    "os.chdir(\"/media/AURA/Documents/JobsApplications/insightdata/nlp/demo/consumer_financial_protection/\")\n",
    "\n",
    "# NLP function to replace english contractions\n",
    "def decontracted(phrase):\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "# function to do some basic NLP pre-processing steps: replacing contractions, stemming words, removing stop words\n",
    "def nlp_preprocess(text_column, # column in Panda table with text\n",
    "                   stop_words, # list of English stopwords\n",
    "                   word_clip = 300): # truncate the number of words\n",
    "   # remove contractions\n",
    "   ps = PorterStemmer()  # stemmer    \n",
    "   cTextl = [decontracted(x) for x in text_column.values.tolist()]\n",
    "   # remove double spacing and non-alphanumeric characters\n",
    "   cTextl=[re.sub(' +',' ',re.sub(r'\\W+', ' ', x)) for x in cTextl]\n",
    "   # lower case the words\n",
    "   cTextl = [x.lower() for x in cTextl]\n",
    "   # stop words and stemming\n",
    "   for i in range(0,len(cTextl)):\n",
    "      rawtext = cTextl[i].split(\" \") # splits sentence by spaces\n",
    "      rawtext = rawtext[0:min(word_clip,len(rawtext))] # take only 300 words maximum\n",
    "      # stem and remove stopwords in one line (expensive operation)\n",
    "      newtext = \" \".join(ps.stem(word) for word in rawtext if not word in stop_words)  # loop through words, stem,join\n",
    "      cTextl[i] = newtext\n",
    "   return pd.DataFrame(cTextl)\n",
    "\n",
    "# function: import and pre-process the data (prepare for Keras)\n",
    "def import_and_clean_data(filename, # file name of data to import (either a .csv or a tar.xz file of a .csv)\n",
    "                          col_label = \"Label3\",\n",
    "                          data_dir = \"data/\", # directory\n",
    "                          tmp_dir = \"/tmp/\", # if file is a .tar.xz, where to temporarily extract the data (Windows users need specify differently than /tmp/\n",
    "                          rare_categories_cutoff = 10, # threshold for categories to be included in the training set\n",
    "                          word_clip = 300): # max number of words in text to accept (only first 300 words are retained\n",
    "   # check\n",
    "   if \"tar.xz\" in filename:\n",
    "      print(\"decompressing \" + filename + \" into \"+tmp_dir)\n",
    "      # command for shell\n",
    "      os_system_command = \"tar xf \"+data_dir+filename+\" -C \"+tmp_dir\n",
    "      print(os_system_command)\n",
    "      # run decompression command (for Linux/Mac)\n",
    "      os.system(os_system_command)\n",
    "      newfilename = tmp_dir + filename.split(\".tar.xz\")[0]\n",
    "   else:\n",
    "      print(\"importing csv called \" + filename)\n",
    "      newfilename = data_dir + filename\n",
    "   # read the complaint data \n",
    "   d_raw = pd.read_csv(newfilename, usecols = ['State','Complaint ID','Consumer complaint narrative','Product', 'Sub-product', 'Issue', 'Sub-issue'])\n",
    "   print(\"imported \" + str(d_raw.shape[0]) + \" rows of data\") # notice 191829 rows and 7 columns\n",
    "   # fill NaN with blanks\n",
    "   for col_ in ['Product','Sub-product','Issue']:\n",
    "      d_raw[col_] = d_raw[col_].fillna(\" \") # fill NaN with a character\n",
    "   # factorize the two levels (Product and Product+Issue) to get unique values\n",
    "   d_raw['Label1'] = pd.factorize(d_raw['Product'])[0]\n",
    "   # combine Product + Issues\n",
    "   d_raw['Label3'] = pd.factorize(d_raw['Product'] + d_raw['Sub-product']+d_raw['Issue'])[0] # 570 Categories\n",
    "   # Dictionary: category integers vs. category names\n",
    "   cats = [pd.factorize(d_raw['Product'])[1],  pd.factorize(d_raw['Product'] + d_raw['Sub-product'])[1], pd.factorize(d_raw['Product'] + d_raw['Sub-product']+d_raw['Issue'])[1]]\n",
    "   # truncate the data: only use categories with at least 10 observations\n",
    "   labels_counts = d_raw.groupby([col_label]).size() # counts of Level3 categories \n",
    "   which_labels = np.where(labels_counts>=rare_categories_cutoff)[0] # which categories have at least 'cutoff'\n",
    "   # make new (truncated) dataset\n",
    "   ixSubset = d_raw.Label3.isin(which_labels) # subset integers\n",
    "   # new dataset 'd', as subset of d_raw\n",
    "   d = (d_raw[ixSubset]).copy()\n",
    "   # NLP pre-processing: stopwords removal, stemming, etc.\n",
    "   # get the default English stopwords from nlkt pacakge\n",
    "   from nltk.corpus import stopwords \n",
    "   stop_words = set(stopwords.words('english')) # list of stopwords to remove\n",
    "   # get the stemming object from nltk\n",
    "   ps = PorterStemmer()  # stemmer\n",
    "   # column in data with the Text data (to feed into the LSTM)\n",
    "   col_text = 'Consumer complaint narrative' # name of the column with the text \n",
    "   # NLP: pre-process the text/complaints\n",
    "   print(\"performing NLP pre-processing on column \" + col_text + \" (remove stop words, stemming,...). This may take a while...\")\n",
    "   cText = nlp_preprocess(d[col_text],stop_words, word_clip = word_clip)\n",
    "   print(\"Done NLP pre-processing\")\n",
    "   # process the labels, make into a N-hot-coding matrix \n",
    "   Y = pd.get_dummies(d['Label3'].values) # one-hot coding\n",
    "   # get integers representing each (label3) class (these are the column names)\n",
    "   Ynames_int = Y.columns # notice the confusing mapping of different integers to different integers\n",
    "   # get english issue labels corresponding to each integer value in Ynames_int\n",
    "   Ynames_char = [cats[2][i] for i in Ynames_int] # actual names\n",
    "   # Finally, convert Y into a numpy matrix (not a panda df)\n",
    "   Y = Y.values\n",
    "   print(\"returning cleaned text data 'cText' for use in tokenization; 'Y' as an one-hot-coding matrix of categories; and 'cats' a dictionary matches columns in Y to english category names\")\n",
    "   if \"tar.xz\" in filename:\n",
    "      print(\"deleting temporary file \" + filename)\n",
    "      os_system_command = \"rm \"+ newfilename\n",
    "      print(os_system_command)\n",
    "      os.system(os_system_command)\n",
    "      print(\"done pre-processing\")\n",
    "   return cText, Y, cats\n",
    "\n",
    "# function to calculate sample_weights for keras argument sample_weight\n",
    "def get_class_weights(Y, # N-hot-coding response matrix\n",
    "                      clip_ = 100000): # maximum weight for rarer cases\n",
    "   weights_total_class_counts = (Y).sum(axis=0)\n",
    "   weights_by_class = (min(weights_total_class_counts)/weights_total_class_counts) # weight by the rarest case\n",
    "   Y_int = np.argmax(Y,axis=1)\n",
    "   vWeights_raw = np.array([weights_by_class[i] for i in Y_int], dtype=float)\n",
    "   vWeights = np.clip(vWeights_raw * (Y.shape[0]/sum(vWeights_raw)),0,clip_)\n",
    "   return vWeights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions: Hyperparameter Sampling\n",
    "\n",
    "With 4 hyperparameters and 3 discrete values each, there are 81 possible _combinations_ of hyperparameters. In reality, for a production/research project you'd likely want more fine-grained hyperparameter values, making it inefficient to do cross-validation on EVERY combination. Instead, we'll do a <b>stochastic search</b> across the space of hyperparameter combinations.\n",
    "\n",
    "The goal is to find the best combination of hyperparameter values, much earlier than trying all 81 combinations. But how?\n",
    "\n",
    "Stochastic exploration of the hyperparameter space requires some way to calculate _probabilities_ for each combination of hyperparameters. I use a simple principle from Thompson sampling: the more _uncertain_ a potential action/reward is, the more _likely_ we should try it, and thus reduce our uncertainty and quickly find the highest rewarding action. Here _action_ means picking a combination of hyperparameters to estimate their Expected Loss; and _reward_ is the Expected Loss, or, finding the lowest Expected Loss.\n",
    "\n",
    "The key is how to estimate the probabilities of sampling each combination of hyperparameter: this will be acheive through estimation by an ensemble of regressors (ridge-regression and a decision tree). The regressor will estimate the Expected Loss using the hyperparameter variables as predictor variables. Using a technique of 'leave-on-out' subsampling, we can turn the <b>stability</b> of these estimates into probabilities over the space of hyperparameter combinations. Then, we simply use these probabilities over the hyperparameter space to sample new combinations: higher probability/more stable estimates will be more likely to be picked.\n",
    "\n",
    "There are a bunch of functions, the main ones pertaining to the hyperparameters are:\n",
    " + `make_hyperparameters_combos` : takes a dictionary of hyperparameters and their values and makes a grid of all possible combinations\n",
    " + `optimal_order_of_hyperparameter_runs` : finds the optimal order in which to test the different combination of hyperparameter values. This will find high-contrasting parameter-combinations, so that the space of hyperparameter values is quickly explored (prior to evoking the multi-arm bandit algorithm)\n",
    " + `proposal_hyperparam`: main Thompson sampler; proposes new combinations of hyperparameters for running in cross-validation. Switches between a deterministic algorithm and a stochastic algorithm after a preset number of iterations (`toggle_learner`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# internal function for cross-validation: creates all combinations of hyperparameters\n",
    "def make_hyperparameters_combos(hyper_parameters):\n",
    "   # hyperparmeters' values\n",
    "   hyp_args = [x[1] for x in hyper_parameters.items()]\n",
    "   # names of hyperparameters\n",
    "   hyp_names = [x[0] for x in hyper_parameters.items()]\n",
    "   # all combos of hyperparameters, as a tensor\n",
    "   hyp_args_grid = np.meshgrid(*hyp_args)\n",
    "   # dimensions of the hyperparameters\n",
    "   hyp_args_dimensions = list(hyp_args_grid[0].shape)\n",
    "   # total number of hyperparameter combos\n",
    "   total_combos = round(exp(sum(map(log,hyp_args_dimensions))))\n",
    "   # reshape into a [n_combos,parameters]\n",
    "   hyp_grid = (np.array(hyp_args_grid).reshape(len(hyp_args_grid),total_combos)).T\n",
    "   # convert grid to data.frame\n",
    "   hyp_pd = pd.DataFrame(hyp_grid, columns = hyp_names)\n",
    "   # also make a companion sequence\n",
    "   hyp_seq = [[j for j in range(0,len(x[1]))] for x in hyper_parameters.items()]\n",
    "   hyp_seq_grid = (np.array(np.meshgrid(*hyp_seq)).reshape(len(hyp_args_grid),total_combos)).T\n",
    "   # make an empty panda data.frame to fill with results\n",
    "   empty_res = pd.DataFrame({\"cv_loss\": [0 for i in range(0,hyp_pd.shape[0])], \"best_epoch\": [0 for i in range(0,hyp_pd.shape[0])]})\n",
    "   return hyp_pd, hyp_seq_grid, empty_res\n",
    "\n",
    "# internal function for cross-validation: optimal order of running different hyperparameter scenarios: calculates a 'scenario distance' by finding scenarios that are maximally contrasting with each other\n",
    "def optimal_order_of_hyperparameter_runs(hyp_seq_grid):\n",
    "   scenario_dist = -2 * np.dot(hyp_seq_grid, hyp_seq_grid.T) + np.sum(hyp_seq_grid**2, axis=1) + np.sum(hyp_seq_grid**2, axis=1)[:, np.newaxis]\n",
    "   scenario_rows = [0] # start with row 1\n",
    "   for i in range(0,hyp_seq_grid.shape[0]):\n",
    "      cur_dist = -2 * np.dot(hyp_seq_grid, hyp_seq_grid[scenario_rows].T) + np.sum(hyp_seq_grid[scenario_rows]**2, axis=1) + np.sum(hyp_seq_grid**2, axis=1)[:, np.newaxis]\n",
    "      elem_rank_by_distance = rd(-(((cur_dist**2)).sum(axis=1))**(0.5)).argsort()\n",
    "      pos_elem = [x for x in elem_rank_by_distance if x not in scenario_rows]\n",
    "      if len(pos_elem)>0:\n",
    "         scenario_rows.append(pos_elem[0])\n",
    "   return scenario_rows\n",
    "\n",
    "# internal function for cross-validation: make validation sets and test sets\n",
    "def make_cv_weights(n_obs, fHoldoutProportion = 0.5, kfold = 3, seed = 1000):\n",
    "   # trainging set and test set\n",
    "   ix_train, ix_test = train_test_split([i for i in range(0,n_obs)], test_size = fHoldoutProportion, random_state = seed)\n",
    "   # divide the training data into cross-validation sets\n",
    "   cv_splitter = KFold(n_splits = kfold)\n",
    "   cv_sets = []\n",
    "   for ix_insample, ix_validation in cv_splitter.split(ix_train):\n",
    "      cv_sets.append([ix_insample, ix_validation])\n",
    "   return ix_train, ix_test, cv_sets\n",
    "\n",
    "# internal function for cross-validation: propose hyperparameters, by two methods:\n",
    "# ... i) sequential (just loops through combinations)\n",
    "# ... ii) thompson sampling / multi-arm bandit reinforcement learner (ridge regression & decision trees to try to predict what might be best hyperparameters)\n",
    "def proposal_hyperparam(run_number, # iteration number for the algorithm\n",
    "                        cv_results, # current results of the cv-loss (panda frame)\n",
    "                        hyp_pd, # output from \"make_hyperparameters_combos\" function\n",
    "                        hyp_optimal, # output from \"make_hyperparameters_combos\" function\n",
    "                        toggle_learner = 10, # when to switch to mult-arm bandit learning\n",
    "                        ridge_alpha = 7, # ridge regression learner: shrinkage parameter\n",
    "                        max_depth = 2, # tree-learner maximum tree depth\n",
    "                        multinomial_prior = 1): # diffuse prior on the model space for the multi-arm bandit \n",
    "   n_models = len(hyp_optimal)\n",
    "   if (run_number < toggle_learner) | (run_number > (hyp_pd.shape[0]-1)):\n",
    "      # just sequentially loop through scenarios\n",
    "      print(\"next hyperparameter: estimated by maximum parameter contrast\")\n",
    "      return_hypIx = hyp_optimal[run_number]\n",
    "      return_hyperparameters = hyp_pd.iloc[return_hypIx]\n",
    "   else:\n",
    "      print(\"next hyperparameter: Thompson sampling of hyperparameters based on a multi-arm bandit learner\")      \n",
    "      which_done = np.where(cv_results['best_epoch'].values >0)\n",
    "      which_notdone = np.where(cv_results['best_epoch'].values == 0)\n",
    "      # train ridge regression \n",
    "      learner1 = Ridge(alpha=ridge_alpha, copy_X = True,normalize=True)\n",
    "      learner2 = DecisionTreeRegressor(max_depth = max_depth)\n",
    "      # multi-arm bandit: use leave-one-out estimation to get probabilities of each scenario being the best\n",
    "      counts_best_loss = np.zeros(len(which_notdone[0])) + multinomial_prior # notice shrinkage parameter multinomial_prior=1\n",
    "      for j in range(0,len(which_done[0])):\n",
    "         # drop one observation\n",
    "         loo = which_done[0][np.arange(len(which_done[0]))!=j]\n",
    "         # fit learners\n",
    "         lr1 = learner1.fit(X = hyp_pd.values[loo], y = cv_results['cv_loss'].values[loo]) \n",
    "         lr2 = learner2.fit(X = hyp_pd.values[loo], y = cv_results['cv_loss'].values[loo]) \n",
    "         # predict which will be the lowest loss         \n",
    "         pred_loss1 = lr1.predict(hyp_pd.values[which_notdone]) # expected loss from the learner\n",
    "         pred_loss2 = lr2.predict(hyp_pd.values[which_notdone]) # expected loss from the learner         \n",
    "         # increment number of time's each scenario is predicted to be the best\n",
    "         counts_best_loss += 0.5*(pred_loss1 == min(pred_loss1))\n",
    "         counts_best_loss += 0.5*(pred_loss2 == min(pred_loss2)) \n",
    "      # convert frequency of each scenario being the best\n",
    "      thompson_sampling_prob = counts_best_loss/sum(counts_best_loss)\n",
    "      # Thompson sampler: random sample from the probabilities\n",
    "      return_hypIx = np.random.choice(which_notdone[0],p=thompson_sampling_prob)\n",
    "      # get the index of the hyperparameters for the best expectd loss\n",
    "      # hyperparameters for the best expectd loss\n",
    "      return_hyperparameters = hyp_pd.iloc[return_hypIx]\n",
    "   return return_hypIx, return_hyperparameters \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
