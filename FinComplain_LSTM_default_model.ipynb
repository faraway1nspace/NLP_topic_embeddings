{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "This notebook presents an LSTM model for supervised text-classification of consumer complaints. The model takes the written complaints of financial products from US consumers and classifies the them into various financial categories (e.g., mortgage fraud, undue fees, identity theft).\n",
    "\n",
    "The model is based on a similar analysis I performed during an Insight Data fellowship using a proprietary data for classification and sentiment analysis.\n",
    "\n",
    "In this demo, we will design a <b>Long Short Term Memory</b> (LSTM) deep-learning model using the Keras API. It is meant as a benchmark for another model in the sibling file XXX, which uses another technique. The following analysis may stand on its own, although I would suggest users try and take inspiration from the other file.\n",
    "\n",
    "## Analysis\n",
    "In the following script, we will do the following:\n",
    "\n",
    "+ some basic NLP preprocessing\n",
    "+ design and execute an LSTM\n",
    "+ validation\n",
    "+ hyperparameter tuning\n",
    "\n",
    "## Data\n",
    "Data is included in the 'data' directory as a large compressed .csv file from the US Consumer Financial Protection Bureau [available here](https://www.consumerfinance.gov/data-research/consumer-complaints/search/?from=0&searchField=all&searchText=&size=25&sort=created_date_desc). To save space, I keep the data as a tar xz file, and in the python code I extract it for one-time use in a tmp directory. This works on Linux/Mac, but windows users may have to manually extract the contents, and modify the script to import from where they extracted the contents. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Part 1: Setup and NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from keras import backend as tf\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Embedding, LSTM, RepeatVector, concatenate, Dense, Reshape, Flatten\n",
    "from keras.models import Model \n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "from array import array\n",
    "\n",
    "# set the working directory\n",
    "# os.chdir(\".\") # generally a good practise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data (extraction and import)\n",
    "The following decompresses the tar.xz file, calling the Unix function tar through os.system. If this doesn't work for you, just manully navigate to the data directory and decompress the .tar.xz file using whatever program you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"tar xf data/complaints-2018-09-30_17_53.csv.tar.xz -C /data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data file\n",
    "data_dir = \"data/\" # I use /tmp/\n",
    "fname = \"complaints-2018-09-30_17_53.csv.tar.xz\"\n",
    "f = data_dir + fname\n",
    "\n",
    "# read the complaint data \n",
    "d_raw = pd.read_csv(f, usecols = ['State','Complaint ID','Consumer complaint narrative','Product', 'Sub-product', 'Issue', 'Sub-issue'])\n",
    "d_raw.shape # notice 191829 rows and 7 columns\n",
    "\n",
    "# fill NaN with blanks\n",
    "for col_ in ['Product','Sub-product','Issue']:\n",
    "   d_raw[col_] = d_raw[col_].fillna(\" \") # fill NaN with a character\n",
    "\n",
    "print(d_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key parts of the data include:\n",
    "+ Consumer complaint narrative: the text of the consumer's complaint\n",
    "+ Product: highest-level of complaint categorization \n",
    "+ Sub-product: 2nd-level of complaint categorization\n",
    "+ Issue: 3rd-level of complaint categorization\n",
    "+ Sub-Issue: ...\n",
    "\n",
    "We will work at the level of <b>Issue</b> (which I will refer to as 'labels3'. There are over 400 sub-issues. \n",
    "\n",
    "The following cell combines Products and Issues, and collects all the unique categorizes. We will also <b>truncate</b> the data to exclude Issues with less than <b>10</b> representatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(191193, 9)\n"
     ]
    }
   ],
   "source": [
    "# factorize the two levels (Product and Prodcut+Issue) to get unique values\n",
    "d_raw['Label1'] = pd.factorize(d_raw['Product'])[0]\n",
    "d_raw['Label3'] = pd.factorize(d_raw['Product'] + d_raw['Sub-product']+d_raw['Issue'])[0] # 570 Categories\n",
    " \n",
    "# Dictionary: category integers vs. category names\n",
    "cats = [pd.factorize(d_raw['Product'])[1], \n",
    "        pd.factorize(d_raw['Product'] + d_raw['Sub-product'])[1], \n",
    "        pd.factorize(d_raw['Product'] + d_raw['Sub-product']+d_raw['Issue'])[1]]\n",
    "\n",
    "# truncate the data: only use categories with at least 10 observations\n",
    "col_label = 'Label3' # columns to use for filtering\n",
    "cutoff = 10 # truncation cutoff\n",
    "labels_counts = d_raw.groupby([col_label]).size() # counts of Level3 categories \n",
    "which_labels = np.where(labels_counts>=cutoff)[0] # which categories have at least 'cutoff'\n",
    "\n",
    "# make new (truncated) dataset\n",
    "ixSubset = d_raw.Label3.isin(which_labels) # subset integers\n",
    "# new dataset 'd', as subset of d_raw\n",
    "d = (d_raw[ixSubset]).copy()\n",
    "\n",
    "# new data set\n",
    "print(d.shape) # vs d_raw.shape\n",
    "print(cats)\n",
    "\n",
    "# del d_raw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "We use some basic NLP techniques to prepare the data for input into the LSTM (the data is already pretty clean, otherwise, your dataset will involve a lot more, like removing non-english respondants, auto-correct):\n",
    "+ remove/replace contractions (e.g., can't vs cannot)\n",
    "+ remove non-alphanumeric characters\n",
    "+ remove double whitespace\n",
    "+ remove stop words\n",
    "+ stemming (e.g., {improvement, improved} = {improv,improv}\n",
    "+ cap the number of words for model\n",
    "\n",
    "<b>WARNING: the stemming takes a long time >2 minutes </b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import download as nltk_downloader \n",
    "\n",
    "# quick function to replace substitutions\n",
    "def decontracted(phrase):\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    " # function to do some basic NLP pre-processing steps: replacing contractions, stemming words, removing stop words\n",
    "def nlp_preprocess(text_column, # column in Panda table with text\n",
    "                   stop_words, # list of English stopwords\n",
    "                   word_clip = 300): # truncate the number of words\n",
    "   # remove contractions\n",
    "   cTextl = [decontracted(x) for x in text_column.values.tolist()]\n",
    "   # remove double spacing and non-alphanumeric characters\n",
    "   cTextl=[re.sub(' +',' ',re.sub(r'\\W+', ' ', x)) for x in cTextl]\n",
    "   # lower case the words\n",
    "   cTextl = [x.lower() for x in cTextl]\n",
    "   # stop words and stemming\n",
    "   for i in range(0,len(cTextl)):\n",
    "      rawtext = cTextl[i].split(\" \") # splits sentence by spaces\n",
    "      rawtext = rawtext[0:min(word_clip,len(rawtext))] # take only 300 words maximum\n",
    "      # stem and remove stopwords in one line (expensive operation)\n",
    "      newtext = \" \".join(ps.stem(word) for word in rawtext if not word in stop_words)  # loop through words, stem,join\n",
    "      cTextl[i] = newtext\n",
    "   return pd.DataFrame(cTextl)\n",
    "\n",
    "# get the default English stopwords from nlkt pacakge\n",
    "stop_words = set(stopwords.words('english')) # list of stopwords to remove\n",
    "# get the stemming object from nltk\n",
    "ps = PorterStemmer()  # stemmer\n",
    "\n",
    "# text column to feed into the LSTM-deep learning model:\n",
    "col_text = 'Consumer complaint narrative' # name of the column with the text \n",
    "\n",
    "# NLP processing function\n",
    "cText = nlp_preprocess(d[col_text],stop_words, word_clip = 300)\n",
    "\n",
    "# show example of the original versus processed data\n",
    "print(\"Original text: \" + d[col_text][0] + \"\\n\") #original \n",
    "print(\"Processed text: \" + cText.iloc[0,0]) # stemmed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize Text Data for LSTM input\n",
    "Having cleanned the text data, we now use the NLTK 'tokenizer' to vectorize the text into integers representing the most common 3000 words. NOTE: 'max_tokens' should be considered a type of <b>hyper-parameter</b>. More words can potentially capture more meaning, or more noise.\n",
    "\n",
    "The matrix of token-sequence (X, below) will be the input for the word-embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(191193, 301)\n"
     ]
    }
   ],
   "source": [
    "# maximum number of words to consider in corpus for embedding (2000-10000 seems the general range. You should treat this like a (coarse) hyperparameter\n",
    "max_tokens = 3000 \n",
    "tokenizer = Tokenizer(num_words=max_tokens, split=' ')\n",
    "tokenizer.fit_on_texts(\"STARTCODON \" + cText[0])\n",
    "# notice the addition of a start codon to signal to the LSTM where the sentence begins (due to the subsequent zero-padding to standardize the length of every tokenized-sentence/sequence)\n",
    "\n",
    "# Model Input: tokenized the text data for input into word-embedding layer\n",
    "X = pad_sequences(tokenizer.texts_to_sequences((\"STARTCODON \" + cText[0]).values)) # tokenize and pad with zeros\n",
    "\n",
    "# number of observations/rows\n",
    "n_obs = X.shape[0] \n",
    "\n",
    "# notice the shape: 300 tokens per complaint (standardized with zero-padding)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response Variable (Y): N-hot-coding\n",
    "The response variable will be a matrix of one-hot-codings for all the different types of 'Issues'/label3 categories. In this set, there are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Output: multinomial & multiclass labels (level 3)\n",
    "Y = pd.get_dummies(d['Label3'].values) # one-hot coding\n",
    "\n",
    "# get integers representing each (label3) class (these are the column names)\n",
    "Ynames_int = Y.columns # notice the confusing mapping of different integers to different integers\n",
    "\n",
    "# get english issue labels corresponding to each integer value in Ynames_int\n",
    "Ynames_char = [cats[2][i] for i in Ynames_int] # actual names\n",
    "\n",
    "# Finally, convert Y into a numpy matrix (not a panda df)\n",
    "Y = Y.values\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
