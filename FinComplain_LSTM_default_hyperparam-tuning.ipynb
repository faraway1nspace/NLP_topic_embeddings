{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-Parameter Tuning (default model)\n",
    "This document tunes hyperparameters for use in the \"Default model\" that performs text classification of consumer complaints (using NLP-LSTM mode; see script [here](https://github.com/faraway1nspace/NLP_topic_embeddings/blob/master/FinComplain_LSTM_default_model.ipynb)).\n",
    "\n",
    "<b>Hyperparameters</b> should be done by minimizing a cross-validation loss. In this script, I use a <b>Thompson-sampler</b> (inspired by a cruide _reinforcement-learning_ / multi-arm bandit algorithm), to stochastically explore the high-dimensional hyperparameter space and find good hyperparameter values. This is an original algorithm and is untested, but it seems to work well.\n",
    "\n",
    "The hyperparameters include:\n",
    "+ the embedding dimension of the word-embedding (like word2vec) \n",
    "+ the size of the corpus used in the word-tokenization (which feeds into the word-embedding). \n",
    "+ the dimensionality of the Long-Short-Term-memory outputs\n",
    "+ the Dropout rate in the LSTM\n",
    "\n",
    "... as well as choosing total number of epochs to run the LSTM.\n",
    "\n",
    "### Function overview\n",
    "+ `run_model` The main function which runs an individual cross-validation run \n",
    "+ `proposal_hyperparam` The main thompson sampler function which proposes new samples from the hyperparameter space.\n",
    "\n",
    "The procedure is straight forward\n",
    "+ get <b>samples</b> from the hyperparameter space\n",
    "+ do <b>3-fold cross-validation</b> to estimate an Expected Loss (aka hold-out loss)\n",
    "+ use the Expected Loss as the 'reward' in a <b>multi-arm bandit learner</b>\n",
    "+ calculate <b>probabilities</b> for each combination of hyperparameters\n",
    "+ use the <b>Thompson-sampler</b>/multi-arm bandit algorithm to draw a new sample from the hyperparameter space\n",
    "+ <b>repeat</b> for about 30 iterations.\n",
    "\n",
    "The multi-arm bandit learner should progressively sample from the hyperparameter space that has a higher-probability of minimizing the Expected Loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions: Data Import & Natural Language Pre-Processing\n",
    "\n",
    "The following functions are some idiosyncratic functions to import and clean the Financial complaint data. For the background of the data source and the models' purpose, please see the [Readme file](https://github.com/faraway1nspace/NLP_topic_embeddings) as well as the [US Consumer Complaint Database](https://www.consumerfinance.gov/data-research/consumer-complaints/) at the US Consumer Financial Protection Bureau website.\n",
    "+ `import_and_clean_data` : reads the complaint data and organizes it for the `keras` LSTM model\n",
    "+ `nlp_preprocess` \" does some basic NLP pre-processing (stemming, removing stop words, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from math import log,exp\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import download as nltk_downloader \n",
    "from keras import backend as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Embedding, LSTM, RepeatVector, concatenate, Dense, Reshape, Flatten\n",
    "from keras.models import Model\n",
    "from scipy.stats import rankdata as rd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "from sklearn.model_selection import KFold # import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# set the working directory\n",
    "os.chdir(\"/media/AURA/Documents/JobsApplications/insightdata/nlp/demo/consumer_financial_protection/\")\n",
    "\n",
    "# NLP function to replace english contractions\n",
    "def decontracted(phrase):\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "# function to do some basic NLP pre-processing steps: replacing contractions, stemming words, removing stop words\n",
    "def nlp_preprocess(text_column, # column in Panda table with text\n",
    "                   stop_words, # list of English stopwords\n",
    "                   word_clip = 300): # truncate the number of words\n",
    "   # remove contractions\n",
    "   ps = PorterStemmer()  # stemmer    \n",
    "   cTextl = [decontracted(x) for x in text_column.values.tolist()]\n",
    "   # remove double spacing and non-alphanumeric characters\n",
    "   cTextl=[re.sub(' +',' ',re.sub(r'\\W+', ' ', x)) for x in cTextl]\n",
    "   # lower case the words\n",
    "   cTextl = [x.lower() for x in cTextl]\n",
    "   # stop words and stemming\n",
    "   for i in range(0,len(cTextl)):\n",
    "      rawtext = cTextl[i].split(\" \") # splits sentence by spaces\n",
    "      rawtext = rawtext[0:min(word_clip,len(rawtext))] # take only 300 words maximum\n",
    "      # stem and remove stopwords in one line (expensive operation)\n",
    "      newtext = \" \".join(ps.stem(word) for word in rawtext if not word in stop_words)  # loop through words, stem,join\n",
    "      cTextl[i] = newtext\n",
    "   return pd.DataFrame(cTextl)\n",
    "\n",
    "# function: import and pre-process the data (prepare for Keras)\n",
    "def import_and_clean_data(filename, # file name of data to import (either a .csv or a tar.xz file of a .csv)\n",
    "                          col_label = \"Label3\",\n",
    "                          data_dir = \"data/\", # directory\n",
    "                          tmp_dir = \"/tmp/\", # if file is a .tar.xz, where to temporarily extract the data (Windows users need specify differently than /tmp/\n",
    "                          rare_categories_cutoff = 10, # threshold for categories to be included in the training set\n",
    "                          word_clip = 300): # max number of words in text to accept (only first 300 words are retained\n",
    "   # check\n",
    "   if \"tar.xz\" in filename:\n",
    "      print(\"decompressing \" + filename + \" into \"+tmp_dir)\n",
    "      # command for shell\n",
    "      os_system_command = \"tar xf \"+data_dir+filename+\" -C \"+tmp_dir\n",
    "      print(os_system_command)\n",
    "      # run decompression command (for Linux/Mac)\n",
    "      os.system(os_system_command)\n",
    "      newfilename = tmp_dir + filename.split(\".tar.xz\")[0]\n",
    "   else:\n",
    "      print(\"importing csv called \" + filename)\n",
    "      newfilename = data_dir + filename\n",
    "   # read the complaint data \n",
    "   d_raw = pd.read_csv(newfilename, usecols = ['State','Complaint ID','Consumer complaint narrative','Product', 'Sub-product', 'Issue', 'Sub-issue'])\n",
    "   print(\"imported \" + str(d_raw.shape[0]) + \" rows of data\") # notice 191829 rows and 7 columns\n",
    "   # fill NaN with blanks\n",
    "   for col_ in ['Product','Sub-product','Issue']:\n",
    "      d_raw[col_] = d_raw[col_].fillna(\" \") # fill NaN with a character\n",
    "   # factorize the two levels (Product and Product+Issue) to get unique values\n",
    "   d_raw['Label1'] = pd.factorize(d_raw['Product'])[0]\n",
    "   # combine Product + Issues\n",
    "   d_raw['Label3'] = pd.factorize(d_raw['Product'] + d_raw['Sub-product']+d_raw['Issue'])[0] # 570 Categories\n",
    "   # Dictionary: category integers vs. category names\n",
    "   cats = [pd.factorize(d_raw['Product'])[1],  pd.factorize(d_raw['Product'] + d_raw['Sub-product'])[1], pd.factorize(d_raw['Product'] + d_raw['Sub-product']+d_raw['Issue'])[1]]\n",
    "   # truncate the data: only use categories with at least 10 observations\n",
    "   labels_counts = d_raw.groupby([col_label]).size() # counts of Level3 categories \n",
    "   which_labels = np.where(labels_counts>=rare_categories_cutoff)[0] # which categories have at least 'cutoff'\n",
    "   # make new (truncated) dataset\n",
    "   ixSubset = d_raw.Label3.isin(which_labels) # subset integers\n",
    "   # new dataset 'd', as subset of d_raw\n",
    "   d = (d_raw[ixSubset]).copy()\n",
    "   # NLP pre-processing: stopwords removal, stemming, etc.\n",
    "   # get the default English stopwords from nlkt pacakge\n",
    "   from nltk.corpus import stopwords \n",
    "   stop_words = set(stopwords.words('english')) # list of stopwords to remove\n",
    "   # get the stemming object from nltk\n",
    "   ps = PorterStemmer()  # stemmer\n",
    "   # column in data with the Text data (to feed into the LSTM)\n",
    "   col_text = 'Consumer complaint narrative' # name of the column with the text \n",
    "   # NLP: pre-process the text/complaints\n",
    "   print(\"performing NLP pre-processing on column \" + col_text + \" (remove stop words, stemming,...). This may take a while...\")\n",
    "   cText = nlp_preprocess(d[col_text],stop_words, word_clip = word_clip)\n",
    "   print(\"Done NLP pre-processing\")\n",
    "   # process the labels, make into a N-hot-coding matrix \n",
    "   Y = pd.get_dummies(d['Label3'].values) # one-hot coding\n",
    "   # get integers representing each (label3) class (these are the column names)\n",
    "   Ynames_int = Y.columns # notice the confusing mapping of different integers to different integers\n",
    "   # get english issue labels corresponding to each integer value in Ynames_int\n",
    "   Ynames_char = [cats[2][i] for i in Ynames_int] # actual names\n",
    "   # Finally, convert Y into a numpy matrix (not a panda df)\n",
    "   Y = Y.values\n",
    "   print(\"returning cleaned text data 'cText' for use in tokenization; 'Y' as an one-hot-coding matrix of categories; and 'cats' a dictionary matches columns in Y to english category names\")\n",
    "   if \"tar.xz\" in filename:\n",
    "      print(\"deleting temporary file \" + filename)\n",
    "      os_system_command = \"rm \"+ newfilename\n",
    "      print(os_system_command)\n",
    "      os.system(os_system_command)\n",
    "      print(\"done pre-processing\")\n",
    "   return cText, Y, cats\n",
    "\n",
    "# function to calculate sample_weights for keras argument sample_weight\n",
    "def get_class_weights(Y, # N-hot-coding response matrix\n",
    "                      clip_ = 100000): # maximum weight for rarer cases\n",
    "   weights_total_class_counts = (Y).sum(axis=0)\n",
    "   weights_by_class = (min(weights_total_class_counts)/weights_total_class_counts) # weight by the rarest case\n",
    "   Y_int = np.argmax(Y,axis=1)\n",
    "   vWeights_raw = np.array([weights_by_class[i] for i in Y_int], dtype=float)\n",
    "   vWeights = np.clip(vWeights_raw * (Y.shape[0]/sum(vWeights_raw)),0,clip_)\n",
    "   return vWeights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions: Hyperparameter Sampling\n",
    "\n",
    "With 4 hyperparameters and (at least) 3 discrete values each, I explored 81 possible _combinations_ of hyperparameter (for an academic or production-level project, you'd likely want more fine-grained hyperparameter values). The goal is to find the best combination of hyperparameter values, based on a cross-validation loss (Expected Loss). It is very inefficient to do cross-validation on EVERY combination. Instead, we'll do a <b>stochastic search</b> across the space of hyperparameter combinations, arriving at the best combination much earlier than running all 81 combinations.\n",
    "\n",
    "#### Thompson sampling:\n",
    "Stochastic exploration of the hyperparameter space requires some way to calculate _probabilities_ for each combination of hyperparameters. I use a simple principle from reinforcement learning called <b>Thompson sampling</b>: the more _uncertain_ a potential action/reward is, the more _likely_ we should try it, and thus reduce our uncertainty quickly find the most rewarding action. Here _action_ means picking a combination of hyperparameters to estimate the Expected Loss; and _reward_ is finding the lowest Expected Loss.\n",
    "\n",
    "#### Model uncertainty as a Dirichlet-Multinomial process\n",
    "The key innovation I present is my method to estimate the probabilities for each hyperparameter combination: this will involve an ensemble of penalized-regressors (`Ridge` and `DecisionTreeRegressor` from `sklearn`). The regressors will estimate the Expected Loss of each combination using the hyperparameter variables as predictor variables. The _frequency counts_ of each hyperparameter-combination being the best model (lowest predicted loss) is converted into a probability via the <b>Dirichlet-Multinomial</b> process. \n",
    "\n",
    "There are a bunch of functions, the main ones pertaining to the hyperparameters are:\n",
    " + `make_hyperparameters_combos` : takes a dictionary of hyperparameters values and makes a grid of all possible combinations\n",
    " + `optimal_order_of_hyperparameter_runs` : proposes an optimal sequence in which to run the different combination of hyperparameter values. This will find high-contrasting parameter-combinations, so that the space of hyperparameter values is quickly explored deterministically, prior to evoking the multi-arm bandit algorithm\n",
    " + `proposal_hyperparam`: main Thompson sampler; proposes new combinations of hyperparameters to run in cross-validation. It switches between a deterministic algorithm and a stochastic algorithm after a preset number of iterations (`toggle_learner`). There are other hyper-hyper-parameters that govern the learning behaviour of the multi-arm-bandit process (`ridge_alpha`, `max_depth`,`multinomial_prior`) and should be left as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# internal function for cross-validation: creates all combinations of hyperparameters\n",
    "def make_hyperparameters_combos(hyper_parameters):\n",
    "   # hyperparmeters' values\n",
    "   hyp_args = [x[1] for x in hyper_parameters.items()]\n",
    "   # names of hyperparameters\n",
    "   hyp_names = [x[0] for x in hyper_parameters.items()]\n",
    "   # all combos of hyperparameters, as a tensor\n",
    "   hyp_args_grid = np.meshgrid(*hyp_args)\n",
    "   # dimensions of the hyperparameters\n",
    "   hyp_args_dimensions = list(hyp_args_grid[0].shape)\n",
    "   # total number of hyperparameter combos\n",
    "   total_combos = round(exp(sum(map(log,hyp_args_dimensions))))\n",
    "   # reshape into a [n_combos,parameters]\n",
    "   hyp_grid = (np.array(hyp_args_grid).reshape(len(hyp_args_grid),total_combos)).T\n",
    "   # convert grid to data.frame\n",
    "   hyp_pd = pd.DataFrame(hyp_grid, columns = hyp_names)\n",
    "   # also make a companion sequence\n",
    "   hyp_seq = [[j for j in range(0,len(x[1]))] for x in hyper_parameters.items()]\n",
    "   hyp_seq_grid = (np.array(np.meshgrid(*hyp_seq)).reshape(len(hyp_args_grid),total_combos)).T\n",
    "   # make an empty panda data.frame to fill with results\n",
    "   empty_res = pd.DataFrame({\"cv_loss\": [0 for i in range(0,hyp_pd.shape[0])], \"best_epoch\": [0 for i in range(0,hyp_pd.shape[0])]})\n",
    "   return hyp_pd, hyp_seq_grid, empty_res\n",
    "\n",
    "# internal function for cross-validation: optimal order of running different hyperparameter scenarios: calculates a 'scenario distance' by finding scenarios that are maximally contrasting with each other\n",
    "def optimal_order_of_hyperparameter_runs(hyp_seq_grid):\n",
    "   scenario_dist = -2 * np.dot(hyp_seq_grid, hyp_seq_grid.T) + np.sum(hyp_seq_grid**2, axis=1) + np.sum(hyp_seq_grid**2, axis=1)[:, np.newaxis]\n",
    "   scenario_rows = [0] # start with row 1\n",
    "   for i in range(0,hyp_seq_grid.shape[0]):\n",
    "      cur_dist = -2 * np.dot(hyp_seq_grid, hyp_seq_grid[scenario_rows].T) + np.sum(hyp_seq_grid[scenario_rows]**2, axis=1) + np.sum(hyp_seq_grid**2, axis=1)[:, np.newaxis]\n",
    "      elem_rank_by_distance = rd(-(((cur_dist**2)).sum(axis=1))**(0.5)).argsort()\n",
    "      pos_elem = [x for x in elem_rank_by_distance if x not in scenario_rows]\n",
    "      if len(pos_elem)>0:\n",
    "         scenario_rows.append(pos_elem[0])\n",
    "   return scenario_rows\n",
    "\n",
    "# internal function for cross-validation: make validation sets and test sets\n",
    "def make_cv_weights(n_obs, fHoldoutProportion = 0.5, kfold = 3, seed = 1000):\n",
    "   # trainging set and test set\n",
    "   ix_train, ix_test = train_test_split([i for i in range(0,n_obs)], test_size = fHoldoutProportion, random_state = seed)\n",
    "   # divide the training data into cross-validation sets\n",
    "   cv_splitter = KFold(n_splits = kfold)\n",
    "   cv_sets = []\n",
    "   for ix_insample, ix_validation in cv_splitter.split(ix_train):\n",
    "      cv_sets.append([ix_insample, ix_validation])\n",
    "   return ix_train, ix_test, cv_sets\n",
    "\n",
    "# internal function for cross-validation: propose hyperparameters, by two methods:\n",
    "# ... i) sequential (just loops through combinations)\n",
    "# ... ii) thompson sampling / multi-arm bandit reinforcement learner (ridge regression & decision trees to try to predict what might be best hyperparameters)\n",
    "def proposal_hyperparam(run_number, # iteration number for the algorithm\n",
    "                        cv_results, # current results of the cv-loss (panda frame)\n",
    "                        hyp_pd, # output from \"make_hyperparameters_combos\" function\n",
    "                        hyp_optimal, # output from \"make_hyperparameters_combos\" function\n",
    "                        toggle_learner = 10, # when to switch to mult-arm bandit learning\n",
    "                        ridge_alpha = 7, # ridge regression learner: shrinkage parameter\n",
    "                        max_depth = 2, # tree-learner maximum tree depth\n",
    "                        multinomial_prior = 1): # diffuse prior on the model space for the multi-arm bandit \n",
    "   n_models = len(hyp_optimal)\n",
    "   if (run_number < toggle_learner) | (run_number > (hyp_pd.shape[0]-1)):\n",
    "      # just sequentially loop through hyperparameter combos\n",
    "      print(\"next hyperparameter: estimated by maximum parameter contrast\")\n",
    "      return_hypIx = hyp_optimal[run_number]\n",
    "      return_hyperparameters = hyp_pd.iloc[return_hypIx]\n",
    "   else:\n",
    "      print(\"next hyperparameter: Thompson sampling of hyperparameters based on a multi-arm bandit learner\") \n",
    "        # find which hyperparam/combos are already completed (used for estimation)\n",
    "      which_done = np.where(cv_results['best_epoch'].values >0)\n",
    "      # find which hyperparam/combos are not yet run (used for predicting their loss)\n",
    "      which_notdone = np.where(cv_results['best_epoch'].values == 0)\n",
    "      # train a ridge regression \n",
    "      learner1 = Ridge(alpha=ridge_alpha, copy_X = True,normalize=True)\n",
    "      # train a decision tree\n",
    "      learner2 = DecisionTreeRegressor(max_depth = max_depth)\n",
    "      # multi-arm bandit: use leave-one-out estimation to get frequency counts that each combo is the best\n",
    "      counts_best_loss = np.zeros(len(which_notdone[0])) + multinomial_prior # notice shrinkage parameter multinomial_prior=1\n",
    "      # loop through each completed combo and drop it (leave-one-out subsampling)\n",
    "      for j in range(0,len(which_done[0])):\n",
    "         # drop the j'th combo (leave-one-out)\n",
    "         loo = which_done[0][np.arange(len(which_done[0]))!=j]\n",
    "         # fit learners\n",
    "         lr1 = learner1.fit(X = hyp_pd.values[loo], y = cv_results['cv_loss'].values[loo]) \n",
    "         lr2 = learner2.fit(X = hyp_pd.values[loo], y = cv_results['cv_loss'].values[loo]) \n",
    "         # predict which will be the lowest loss         \n",
    "         pred_loss1 = lr1.predict(hyp_pd.values[which_notdone]) # expected loss from the learner\n",
    "         pred_loss2 = lr2.predict(hyp_pd.values[which_notdone]) # expected loss from the learner         \n",
    "         # increment number of time's each hypparam/combo is predicted to be the best\n",
    "         counts_best_loss += 0.5*(pred_loss1 == min(pred_loss1))\n",
    "         counts_best_loss += 0.5*(pred_loss2 == min(pred_loss2))\n",
    "      # convert selection frequency of each combo into a probability (Dirichlet)\n",
    "      thompson_sampling_prob = np.random.dirichlet(counts_best_loss)\n",
    "      # Thompson sampler: random sample from the probabilities (Multinomial)\n",
    "      return_hypIx = np.random.choice(which_notdone[0],p=thompson_sampling_prob)\n",
    "      # get the index of the hyperparameters for the best expectd loss\n",
    "      # hyperparameters for the best expectd loss\n",
    "      return_hyperparameters = hyp_pd.iloc[return_hypIx]\n",
    "   return return_hypIx, return_hyperparameters \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions: K-fold Cross-Validation\n",
    "The following functions support the 3-fold cross validation. The are fairly self-explanatory. The main functions are:\n",
    "+ `make_cv_weights` : splits the data into two indices for training & testing. Among the training indices (`ix_train`), it further divides the data into mutually exclusive k-fold cross-validation sets (`cv_sets`). You tune the hyperparameters with the subsets in `cv_sets`; you train a final (tuned) model on the `ix_train`, and you validate the final model on the hold-out set `ix_test`.\n",
    "+ `run_model` : runs an LSTM model using the `keras` API; \n",
    "+ `run_cv_model`: calls `run_model` for one CV-iteration; ONLY returns the validation loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# internal function for cross-validation: make validation sets and test sets\n",
    "def make_cv_weights(n_obs, fHoldoutProportion = 0.5, kfold = 3, seed = 1000):\n",
    "   # trainging set and test set\n",
    "   ix_train, ix_test = train_test_split([i for i in range(0,n_obs)], test_size = fHoldoutProportion, random_state = seed)\n",
    "   # divide the training data into cross-validation sets\n",
    "   cv_splitter = KFold(n_splits = kfold)\n",
    "   cv_sets = []\n",
    "   for ix_insample, ix_validation in cv_splitter.split(ix_train):\n",
    "      cv_sets.append([ix_insample, ix_validation])\n",
    "   return ix_train, ix_test, cv_sets\n",
    "\n",
    "# function: LSTM model, using Keras functional API\n",
    "def run_model(X, # tokenized text data\n",
    "              Y, # N-hot-coding label data\n",
    "              W, # sample weights\n",
    "              X_val, # validation data\n",
    "              Y_val, # validation data\n",
    "              iMaxTokens, # size of word-token corpus (hyperparameter)\n",
    "              n_epoch =20, # number of epochs (hyperparameter)\n",
    "              batch_size=64, # batchsize (hyperparameter)\n",
    "              iDimEmbedLSTM=200, # embedding dimension of word-embedding (hyperp.)\n",
    "              iDimOutLSTM = 100, # LSTM output dimensions (hyperparameter)\n",
    "              fDropout = 0.33, # dropout rate for LSTM inputs (hyperparameter)\n",
    "              fDropout_RNN = 0.5, # RNN dropout rate for LSTM (hyperparameter)\n",
    "              iDim_hidden_nodes_final=100): # size of final hidden layer\n",
    "   lstm_input_layer = Input(shape=(X.shape[1],), dtype='int32', name='lstm_input',) # lstm input\n",
    "   lstm_embed_layer = Embedding(input_dim=iMaxTokens, output_dim=iDimEmbedLSTM, input_length=X.shape[1])(lstm_input_layer) # input_dim = vocab size,\n",
    "   lstm_output = LSTM(iDimOutLSTM, dropout = fDropout, recurrent_dropout = fDropout_RNN)(lstm_embed_layer) # the output has dimension (None, 12)\n",
    "   hidden_layer = Dense(iDim_hidden_nodes_final, activation='relu')(lstm_output)\n",
    "   main_output = Dense(Y.shape[1], activation='softmax', name='main_output')(hidden_layer) # main output for categories\n",
    "   model = Model(inputs=[lstm_input_layer], outputs=[main_output])\n",
    "   model.compile(loss = \"categorical_crossentropy\", optimizer='adam')\n",
    "   history = model.fit({'lstm_input': X}, {'main_output': Y}, epochs = n_epoch, batch_size=batch_size, verbose = 0, sample_weight = W, validation_data=(X_val, Y_val))\n",
    "   return model, history\n",
    "\n",
    "# main function: run individual cv-run; ONLY returns the validation loss (deletes model)\n",
    "def run_cv_model(cv_insample_indices,cv_validation_indices, Y, cText, vWeights, hyperparam, n_epoch = 7, fDropout_RNN = 0.1, batch_size = 64):\n",
    "   n_obs = Y.shape[0] # number of observations/rows\n",
    "   # get hyperparameters\n",
    "   iMaxTokens = int(hyperparam['max_tokens']) # maximum number of words in corpus for embedding\n",
    "   iDimEmbedLSTM = int(hyperparam['DimEmbedLSTM']) # embedding dimensions for words\n",
    "   iDimOutLSTM = int(hyperparam['DimOutLSTM']) # LSTM output dimension\n",
    "   fDropout = hyperparam['Dropout'] # LSTM input dropout regularization\n",
    "   iDim_hidden_nodes_final = (np.linspace(iDimOutLSTM,Y.shape[1]).round().astype(int))[1] # number of    \n",
    "   # tokenize the text\n",
    "   tokenizer = Tokenizer(num_words=iMaxTokens, split=' ')\n",
    "   tokenizer.fit_on_texts(\"STARTCODON \" + cText[0])\n",
    "   # text data: insample and validation\n",
    "   X = pad_sequences(tokenizer.texts_to_sequences((\"STARTCODON \" + cText[0]).values)) # tokenize and pad with zeros\n",
    "   X_insample = X[cv_insample_indices] \n",
    "   X_val = X[cv_validation_indices]\n",
    "   # label data (N-hot-coding): insample and validation\n",
    "   Y_insample = Y[cv_insample_indices]\n",
    "   Y_val = Y[cv_validation_indices]\n",
    "   # training weights\n",
    "   W_insample = vWeights[cv_insample_indices]\n",
    "   # fit the model\n",
    "   model, history = run_model(X_insample, Y_insample,W_insample, X_val,Y_val, iMaxTokens, n_epoch, batch_size, iDimEmbedLSTM, iDimOutLSTM, fDropout, fDropout_RNN, iDim_hidden_nodes_final)\n",
    "   val_loss = history.history['val_loss']\n",
    "   del model\n",
    "   del history\n",
    "   return val_loss\n",
    "\n",
    "# internal function for cross-validation: get the best epoch over k-fold validation\n",
    "def get_best_epoch(cv_losses):\n",
    "      expected_loss_per_epoch = (np.array(cv_losses).T).mean(axis=1)\n",
    "      best_epoch = expected_loss_per_epoch.argmin() + 1\n",
    "      return best_epoch, expected_loss_per_epoch[best_epoch-1]\n",
    "\n",
    "# internal function for cross-validation: get best hyperparameter combinations from CV-loss results\n",
    "def get_best_hyperparameters(cv_results, hyperparameter_grid):\n",
    "   which_done = np.where(cv_results['best_epoch'].values!=0)\n",
    "   which_best = which_done[0][cv_results['cv_loss'].values[which_done[0]].argmin()]\n",
    "   best_epoch = cv_results['best_epoch'][which_best]\n",
    "   return hyperparameter_grid.iloc[which_best], best_epoch \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script: Part 1: import and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and clean the data.\n",
    "# Returns 'cText' (the NLP-preprocessed data) and 'cats' (the dictionary of categories/labels for classification')\n",
    "cText, Y, cats = import_and_clean_data(filename = \"complaints-2018-09-30_17_53.csv.tar.xz\", # filename to import the data,\n",
    "    #filename = \"complaints-2018-09-30_17_53.csv\", # filename to import the data,     \n",
    "                                    col_label = \"Label3\", # label to work with for classification/learning\n",
    "                                    data_dir = \"data/\", # directory\n",
    "                                    tmp_dir = \"/tmp/\", # if file is a .tar.xz, where to temporarily extract the data (Windows users need specify differently than /tmp/\n",
    "                                    rare_categories_cutoff = 10, # threshold for categories to be included in the training set\n",
    "                                    word_clip = 300) # max number of words in text to accept (only first 300 words are retained\n",
    "\n",
    "n_obs = Y.shape[0]\n",
    "\n",
    "# sample weights to correct for unbalanced categories / labels\n",
    "vWeights = get_class_weights(Y,5) # weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script: Part 2: define the hyperparameters and set-up cross-validation\n",
    "### validation sets\n",
    "+ `kfold` : used for k-fold cross-validation; I use 3-fold validation\n",
    "+ `fHoldoutProportion` : used to define the proportion of data not-used for training, but for validating the final model.\n",
    "\n",
    "### hyperparameter space\n",
    "+ `max_tokens`: the size of the corpus used in the word-tokenization (which feeds into the word-embedding).\n",
    "+ `DimEmbedLSTM`:the embedding dimension of the word-embedding (like word2vec) \n",
    "+ `DimOutLSTM` : the dimensionality of the Long-Short-Term-memory outputs\n",
    "+ `Dropout` regularization parameter: dropout rate in the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = 3 # k-fold cross-validation\n",
    "fHoldoutProportion = 0.5 # amount of data to leave aside for (final) hold-out validation\n",
    "\n",
    "# hyperparmeters' and their values\n",
    "hyper_parameters = {'max_tokens': [1500,2000,3000], # number of wor tokens\n",
    "                    'DimEmbedLSTM':[97,194,291], # word embedding dimension\n",
    "                    'DimOutLSTM':[100,150,300], # LSTM output dimension\n",
    "                    'Dropout':[0.20,0.33,0.5]} # dropout proportion for LSTM\n",
    "\n",
    "# hyperparameters: make all possible combinations of \n",
    "hyp_grid, ix_hyp_grid, cv_results = make_hyperparameters_combos(hyper_parameters)\n",
    "optimal_hyp_order = optimal_order_of_hyperparameter_runs(ix_hyp_grid)\n",
    "\n",
    "# training set, testing sets, and cross-validation sets: indicies to divide the data\n",
    "ix_train,ix_test,cv_sets = make_cv_weights(n_obs, fHoldoutProportion = fHoldoutProportion, kfold = kfold, seed = 1000)\n",
    "\n",
    "n_epoch = 10 # number of epochs\n",
    "batch_size = 200 # batchsize\n",
    "pause_seconds = 30 # pause between CV-iterations\n",
    "\n",
    "# big loop to run through hyperparameters\n",
    "max_hyperparameter_runs = min(20, len(optimal_hyp_order)) # maximum number of iterations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script: Part 3: Explore the hyperparameter space (cross-validation)\n",
    "This is the main part of the exercise, consisting of two massive loops. The outer loop iterates through different hyperparameter values, and the inner loop iterates through the K-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0 # iterator, outer loop\n",
    "print(\"running k-fold hyperparameter estimation algorithm for \" + str(max_hyperparameter_runs) + \"iterations\")\n",
    "while i < max_hyperparameter_runs:\n",
    "   print(\"hyperparameter tuning, iteration: \" + str(i))\n",
    "   # proposal new hyperparameters from the space of combinations\n",
    "   ixHyper, current_hyper_param = proposal_hyperparam(i, cv_results, hyp_pd = hyp_grid, hyp_optimal = optimal_hyp_order, toggle_learner = 10, ridge_alpha = 7)\n",
    "   print(current_hyper_param.values)\n",
    "   # do cross-validation\n",
    "   k_loss = [] # validation loss results\n",
    "   # inner loop, cross-validation\n",
    "   for icv, lcv_set in enumerate(cv_sets):\n",
    "      k_loss.append(run_cv_model(lcv_set[0],lcv_set[1], Y, cText, vWeights, current_hyper_param, n_epoch = n_epoch, fDropout_RNN = 0.1, batch_size = batch_size))\n",
    "   # return best mean(validation_loss) at which epoch \n",
    "   best_epoch, k_expected_loss = get_best_epoch(k_loss) # get best epoch and expected loss\n",
    "   cv_results['best_epoch'].iloc[ixHyper] = best_epoch\n",
    "   cv_results['cv_loss'].iloc[ixHyper] = k_expected_loss\n",
    "   print(\"done iteration \"+ str(i)+ \". Sleeping for \" + str(pause_seconds) +\" seconds.\")\n",
    "   time.sleep(pause_seconds)\n",
    "   # repeat\n",
    "   i += 1\n",
    "\n",
    "print(\"done k-fold hyperparameter estimation: hit \" + str(max_hyperparameter_runs) + \" iterations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script: Part 4: Get best hyperparameters (tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the CV results: get best hyperparameters\n",
    "best_hyperparameters, n_epoch = get_best_hyperparameters(cv_results, hyp_grid)\n",
    "cv_results = pd.concat([pd.DataFrame(hyp_grid),cv_results],axis=1)\n",
    "cv_results = cv_results[cv_results['best_epoch']!=0]\n",
    "cv_results.sort_values(by=['cv_loss'])\n",
    "print(cv_results) # print the results of cross-validation\n",
    "\n",
    "# Final best hyperparameters\n",
    "iMaxTokens = int(best_hyperparameters['max_tokens'])\n",
    "iDimEmbedLSTM = int(best_hyperparameters['DimEmbedLSTM'])\n",
    "iDimOutLSTM = int(best_hyperparameters['DimOutLSTM'])\n",
    "fDropout = best_hyperparameters['Dropout']\n",
    "iDim_hidden_nodes_final = (np.linspace(iDimOutLSTM,Y.shape[1]).round().astype(int))[1] # number of\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script: Part 5: Run the final 'tuned' model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit final model\n",
    "tokenizer = Tokenizer(num_words=iMaxTokens, split=' ')\n",
    "tokenizer.fit_on_texts(\"STARTCODON \" + cText[0])\n",
    "# text data: insample and validation\n",
    "X = pad_sequences(tokenizer.texts_to_sequences((\"STARTCODON \" + cText[0]).values)) # tokenize and pad with zeros\n",
    "X_train = X[ix_train];\n",
    "Y_train = Y[ix_train]\n",
    "W_train = vWeights[ix_train]\n",
    "# final model\n",
    "lstm_input_layer = Input(shape=(X_train.shape[1],), dtype='int32', name='lstm_input',) # lstm input\n",
    "lstm_embed_layer = Embedding(input_dim=iMaxTokens, output_dim=iDimEmbedLSTM, input_length=X_train.shape[1])(lstm_input_layer) # input_dim = vocab size,\n",
    "lstm_output = LSTM(iDimOutLSTM, dropout = fDropout, recurrent_dropout = fDropout_RNN)(lstm_embed_layer) # the output has dimension (None, 12)\n",
    "hidden_layer = Dense(iDim_hidden_nodes_final, activation='relu')(lstm_output)\n",
    "main_output = Dense(Y_train.shape[1], activation='softmax', name='main_output')(hidden_layer) # main output for categories\n",
    "model = Model(inputs=[lstm_input_layer], outputs=[main_output])\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer='adam')\n",
    "model.fit({'lstm_input': X_train}, {'main_output': Y_train}, epochs = n_epoch, batch_size=batch_size, verbose = 2, sample_weight = W_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script: Part 6: Test the final model\n",
    "Using the holdout data (indices `ix_test`), we will estimate hold-out statistics, like the ROC/AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# holdout data for final validation\n",
    "X_test = X[ix_test]; \n",
    "Y_test = Y[ix_test]; \n",
    "\n",
    "# validation on test set\n",
    "pwide =  model.predict(X_test) # predicted probability matrix\n",
    "pvec = pwide.flatten() # vectorize the prediction matrix \n",
    "yvec = Y_test.flatten() # vectorize the one-hot-coding classes\n",
    "\n",
    "# calcualte fpr, tpr, and AUC scores\n",
    "global_fpr, global_tpr, threshold = roc_curve(yvec, pvec)\n",
    "global_roc_auc = auc(global_fpr, global_tpr) # \n",
    "print(\"Global AUC score=%f\" % (global_roc_auc))\n",
    "\n",
    "# plot the ROCurve\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic (out-of-sample)')\n",
    "plt.plot(global_fpr, global_tpr, 'purple', label = 'Global CV-AUC = %0.2f' % global_roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "#plt.savefig('img/global_roc_default_model.png')\n",
    "plt.show() # to plot in your own terminal\n",
    "#Global AUC score=0.949081\n",
    "\n",
    "# calculate AUC scores per category\n",
    "roc_topic = [] # container for per-category AUC scores\n",
    "Y_labels_test = Y_test.argmax(axis=1)\n",
    "for i in range(0,Y.shape[1]):\n",
    "    tmptopic_fpr, tmptopic_tpr, threshold = roc_curve(Y_test[:,i].flatten(), pwide[:,i].flatten())\n",
    "    roc_topic.append(auc(tmptopic_fpr, tmptopic_tpr))\n",
    "\n",
    "# average ROC/AUC score overall categories\n",
    "print(\"Average AUC (holdout) score is %f\" % (sum(roc_topic)/len(roc_topic)))\n",
    "\n",
    "n, bins, patches = plt.hist(roc_topic, 20, facecolor='blue', alpha=0.5)\n",
    "plt.xlabel('AUC scores (per category)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(r'cv-AUC scores per category')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
